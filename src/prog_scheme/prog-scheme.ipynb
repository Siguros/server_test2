{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.prog_scheme.utils import generate_target_weights\n",
    "\n",
    "input_size = 100\n",
    "output_size = 50\n",
    "rank = 30\n",
    "dim = input_size * output_size\n",
    "over_sampling = 1\n",
    "x_rand = False\n",
    "\n",
    "batch_size = 3\n",
    "tol = 1e-8\n",
    "max_iter = 1000\n",
    "norm_type = \"fro\"\n",
    "svd_every_k_iter = 5\n",
    "read_noise_std = 0.1\n",
    "update_noise_std = 0.1\n",
    "# generate low rank matrix\n",
    "w_target = generate_target_weights(input_size, output_size, rank)\n",
    "print(w_target[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.configs import FloatingPointRPUConfig, SingleRPUConfig\n",
    "from aihwkit.simulator.configs.devices import (\n",
    "    ConstantStepDevice,\n",
    "    DriftParameter,\n",
    "    ExpStepDevice,\n",
    "    FloatingPointDevice,\n",
    "    IdealDevice,\n",
    "    LinearStepDevice,\n",
    "    SimpleDriftParameter,\n",
    ")\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    InputRangeParameter,\n",
    "    PrePostProcessingParameter,\n",
    "    UpdateParameters,\n",
    ")\n",
    "from aihwkit.simulator.parameters.enums import PulseType\n",
    "from aihwkit.simulator.presets.configs import IdealizedPreset, PCMPreset, ReRamSBPreset\n",
    "from aihwkit.simulator.presets.devices import IdealizedPresetDevice\n",
    "from aihwkit.simulator.tiles import FloatingPointTile\n",
    "\n",
    "from src.core.aihwkit.utils import rpuconf2dict\n",
    "from src.prog_scheme.kalman import DeviceKF, ExpDeviceEKF, LinearDeviceEKF\n",
    "\n",
    "pre_post_cfg = PrePostProcessingParameter(input_range=InputRangeParameter(enable=False))\n",
    "# device_cfg = ExpStepDevice()\n",
    "device_cfg = LinearStepDevice()\n",
    "# device_cfg = IdealDevice()\n",
    "\n",
    "update_cfg = UpdateParameters(pulse_type=PulseType.STOCHASTIC, desired_bl=127)\n",
    "rpuconfig = SingleRPUConfig(update=update_cfg, device=device_cfg)\n",
    "rpuconfig.forward.out_noise = read_noise_std\n",
    "rpuconfig.device.write_noise_std = update_noise_std\n",
    "rpuconfig.device.w_max = 1\n",
    "rpuconfig.device.gamma_up = 0.1\n",
    "rpuconfig.device.gamma_down = 0.1\n",
    "rpuconfig.device.w_min = -1\n",
    "rpuconfig.device.w_max_dtod = 0.01\n",
    "rpuconfig.device.w_min_dtod = 0.01\n",
    "rpuconfig.device.dw_min_std = 0.0\n",
    "rpuconfig.device.mult_noise = False  # additive noise\n",
    "# rpuconfig.forward.inp_res = 0\n",
    "# rpuconfig = IdealizedPreset(update=update_cfg, device=device_cfg, pre_post=pre_post_cfg)\n",
    "\n",
    "rpuconf_dict = rpuconf2dict(rpuconfig, max_depth=2)\n",
    "if rpuconfig.device.__class__ == LinearStepDevice:\n",
    "    device_ekf = LinearDeviceEKF(\n",
    "        dim=dim,\n",
    "        read_noise_std=read_noise_std,\n",
    "        update_noise_std=update_noise_std,\n",
    "        iterative_update=False,\n",
    "        **rpuconf_dict[\"device\"],\n",
    "    )\n",
    "elif rpuconfig.device.__class__ == ExpStepDevice:\n",
    "\n",
    "    device_ekf = None\n",
    "    # device_ekf = ExpDeviceEKF(\n",
    "    #     dim=dim,\n",
    "    #     read_noise_std=read_noise_std,\n",
    "    #     update_noise_std=update_noise_std,\n",
    "    #     iterative_update=True,\n",
    "    #     **rpuconf_dict[\"device\"],\n",
    "\n",
    "    # )\n",
    "else:\n",
    "    device_ekf = None\n",
    "    # raise NotImplementedError\n",
    "\n",
    "fnc = DeviceKF(dim, read_noise_std, update_noise_std)\n",
    "\n",
    "conf = {\n",
    "    **rpuconf_dict,\n",
    "    \"matrix\": {\"input_size\": input_size, \"output_size\": output_size, \"rank\": rank},\n",
    "    \"methods\": {\n",
    "        \"fnc\": None,\n",
    "        \"tolerance\": tol,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"norm_type\": norm_type,\n",
    "        \"svd_every_k_iter\": svd_every_k_iter,\n",
    "        \"read_noise_std\": read_noise_std,\n",
    "        \"update_noise_std\": update_noise_std,\n",
    "        \"w_init\": 0.01,\n",
    "        \"over_sampling\": over_sampling,\n",
    "        \"x_rand\": x_rand,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_ekf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional, Tuple, Union\n",
    "\n",
    "from aihwkit.simulator.tiles.analog import AnalogTile\n",
    "from torch import Tensor, eye\n",
    "from torch.autograd import no_grad\n",
    "from torch.linalg import lstsq\n",
    "\n",
    "\n",
    "class customread_tile(AnalogTile):\n",
    "    @no_grad()\n",
    "    def read_weights_(\n",
    "        self,\n",
    "        apply_weight_scaling: bool = False,\n",
    "        x_values: Optional[Tensor] = None,\n",
    "        x_rand: bool = False,\n",
    "        over_sampling: int = 10,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        \"\"\"Reads the weights (and biases) in a realistic manner\n",
    "        by using the forward pass for weights readout.\n",
    "\n",
    "        Gets the tile weights and extracts the mathematical weight\n",
    "        matrix and biases (if present, by determined by the ``self.analog_bias``\n",
    "        parameter).\n",
    "\n",
    "        The weight will not be directly read, but linearly estimated\n",
    "        using random inputs using the analog forward pass.\n",
    "\n",
    "        Note:\n",
    "\n",
    "            If the tile includes digital periphery (e.g. out scaling),\n",
    "            these will be applied. Thus this weight is the logical\n",
    "            weights that correspond to the weights in an FP network.\n",
    "\n",
    "        Note:\n",
    "            weights are estimated using the ``lstsq`` solver from torch.\n",
    "\n",
    "        Args:\n",
    "            apply_weight_scaling: Whether to rescale the given weight matrix\n",
    "                and populate the digital output scaling factors as\n",
    "                specified in the configuration\n",
    "                :class:`~aihwkit.simulator.configs.MappingParameter`. A\n",
    "                new ``weight_scaling_omega`` can be given. Note that\n",
    "                this will overwrite the existing digital out scaling\n",
    "                factors.\n",
    "\n",
    "            x_values: Values to use for estimating the matrix. If\n",
    "                not given, inputs are standard normal vectors.\n",
    "\n",
    "            over_sampling: If ``x_values`` is not given,\n",
    "                ``over_sampling * in_size`` random vectors are used\n",
    "                for the estimation\n",
    "\n",
    "        Returns:\n",
    "            a tuple where the first item is the ``[out_size, in_size]`` weight\n",
    "            matrix; and the second item is either the ``[out_size]`` bias vector\n",
    "            or ``None`` if the tile is set not to use bias.\n",
    "\n",
    "        Raises:\n",
    "            TileError: in case wrong code usage of TileWithPeriphery\n",
    "        \"\"\"\n",
    "        dtype = self.get_dtype()\n",
    "        if x_values is None:\n",
    "            x_values = eye(self.in_size, self.in_size, device=self.device, dtype=dtype)\n",
    "            if x_rand:\n",
    "                x_values = torch.rand(self.in_size, self.in_size, device=self.device, dtype=dtype)\n",
    "        else:\n",
    "            x_values = x_values.to(self.device)\n",
    "\n",
    "        x_values = x_values.repeat(over_sampling, 1)\n",
    "\n",
    "        # forward pass in eval mode\n",
    "        was_training = self.training\n",
    "        is_indexed = self.is_indexed()\n",
    "        self.eval()\n",
    "        if is_indexed:\n",
    "            self.analog_ctx.set_indexed(False)\n",
    "        y_values = self.forward(x_values)\n",
    "        if was_training:\n",
    "            self.train()\n",
    "        if is_indexed:\n",
    "            self.analog_ctx.set_indexed(True)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            y_values -= self.bias\n",
    "\n",
    "        est_weight = lstsq(x_values, y_values).solution.T.cpu()\n",
    "        weight, bias = self._separate_weights(est_weight)\n",
    "\n",
    "        if self.digital_bias:\n",
    "            bias = self.bias.detach().cpu()\n",
    "\n",
    "        if not apply_weight_scaling:\n",
    "            # we de-apply all scales\n",
    "            alpha = self.get_scales()\n",
    "            if alpha is not None:\n",
    "                alpha = alpha.detach().cpu()\n",
    "                return (weight / alpha.view(-1, 1), bias / alpha if self.analog_bias else bias)\n",
    "        return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnalogTile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from aihwkit.simulator.tiles.analog import AnalogTile\n",
    "\n",
    "atile = customread_tile(output_size, input_size, rpu_config=rpuconfig)  # with periphery\n",
    "atile_dic = {}\n",
    "atile.state_dict(atile_dic)\n",
    "# atile2 = AnalogTile(output_size, input_size, rpu_config=rpuconfig)\n",
    "atile2 = copy.deepcopy(atile)\n",
    "# atile2.load_state_dict(atile_dic, assign=True)\n",
    "# atile3 = AnalogTile(output_size, input_size, rpu_config=rpuconfig)\n",
    "atile3 = copy.deepcopy(atile)\n",
    "atile4 = copy.deepcopy(atile)\n",
    "# atile3.load_state_dict(atile_dic, assign=True)\n",
    "print(atile.tile.get_info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.tiles.periphery import TileWithPeriphery\n",
    "\n",
    "from src.prog_scheme.program_methods import GDP, SVD\n",
    "\n",
    "# enroll the programming methods\n",
    "atile.program_weights = GDP.call_Program_Method.__get__(atile, TileWithPeriphery)\n",
    "atile.read_weights_ = GDP.read_weights_.__get__(atile, TileWithPeriphery)\n",
    "atile.init_setup = GDP.init_setup.__get__(atile, TileWithPeriphery)\n",
    "\n",
    "atile2.program_weights = SVD.call_Program_Method.__get__(atile, TileWithPeriphery)\n",
    "atile2.read_weights_ = SVD.read_weights_.__get__(atile, TileWithPeriphery)\n",
    "atile2.init_setup = SVD.init_setup.__get__(atile, TileWithPeriphery)\n",
    "\n",
    "tiles = [atile, atile2]\n",
    "# tiles.append(atile4) if device_ekf is not None else None\n",
    "method_names = [t.program_weights.__name__ for t in tiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prog_scheme.utils import program_n_log\n",
    "\n",
    "err_lists = program_n_log(tiles, w_target.T, conf.get(\"methods\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.get(\"methods\", {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atile2.tile.target_weights = w.T\n",
    "# atile2.program_weights(**conf.get(\"methods\", {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_singular_values(Ws: tuple[torch.Tensor]):\n",
    "    for w in Ws:\n",
    "        s = torch.linalg.svdvals(w.squeeze())\n",
    "        plt.plot(s)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Singular Value Index\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.title(\"Singular Values of Weight Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [w_target.T - tile_.tile.get_weights() for tile_ in tiles]\n",
    "\n",
    "plot_singular_values(W)\n",
    "print(f\"{norm_type} norm of \\n\")\n",
    "for i, w in enumerate(W):\n",
    "    print(f\"atile{i}: {torch.linalg.matrix_norm(w, ord=norm_type)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for err in err_lists:\n",
    "    plt.semilogy(err)\n",
    "    print(err[-1])\n",
    "# set legend\n",
    "plt.legend(method_names)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(f\"{norm_type} norm of weight error\")\n",
    "plt.title(\"Error vs Iteration @ {}x{}, rank={}\".format(input_size, output_size, rank))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "conf[\"methods\"][\"device_ekf\"] = conf[\"methods\"][\"device_ekf\"].__class__.__name__\n",
    "with wandb.init(project=\"prog-scheme\", entity=\"spk\", config=conf, dir=\"../../logs\") as run:\n",
    "    # Determine the maximum length\n",
    "\n",
    "    max_len = max([len(err_list) for err_list in err_lists])\n",
    "    # Pad the lists with 0s\n",
    "    for err_list in err_lists:\n",
    "        err_list += [None] * (max_len - len(err_list))\n",
    "    # Log the data\n",
    "    for i in range(max_len):\n",
    "        run.log(\n",
    "            {f\"{name}_{norm_type}\": err_list[i] for name, err_list in zip(method_names, err_lists)}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZE UPDATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# assert (atile.initial_weights - atile2.initial_weights).max() == 0\n",
    "optimal_change = (w.T - atile2.initial_weights).flatten()\n",
    "\n",
    "# 데이터 가공\n",
    "actual_updates = atile.actual_weight_updates\n",
    "data = np.array(actual_updates)\n",
    "flattened_data = data.reshape(data.shape[0], -1)\n",
    "cumulative_update = np.cumsum(flattened_data, axis=0)\n",
    "\n",
    "data2 = np.array(atile3.actual_weight_updates)\n",
    "flattened_data2 = data2.reshape(data2.shape[0], -1)\n",
    "cumulative_update2 = np.cumsum(flattened_data2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances\n",
    "distance = []\n",
    "distance2 = []\n",
    "for i in range(len(cumulative_update)):\n",
    "    distance.append(optimal_change - cumulative_update[i])\n",
    "\n",
    "for i in range(len(cumulative_update2)):\n",
    "    distance2.append(optimal_change - cumulative_update2[i])\n",
    "\n",
    "distance = np.array(distance)\n",
    "distance2 = np.array(distance2)\n",
    "concat_distances = np.concatenate((distance, distance2), axis=0)\n",
    "\n",
    "# Apply TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd_result = svd.fit_transform(distance)\n",
    "\n",
    "# Map original data to SVD space\n",
    "\n",
    "svd_gdp = svd_result[0 : max_iter - 1]\n",
    "svd_svd = svd_result[max_iter : max_iter * 2 - 1]\n",
    "\n",
    "# Set grid in SVD result range\n",
    "x = np.linspace(svd_result[:, 0].min(), svd_result[:, 0].max(), 100)\n",
    "y = np.linspace(svd_result[:, 1].min(), svd_result[:, 1].max(), 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Calculate distance from origin in SVD space\n",
    "Z = np.sqrt(X**2 + Y**2)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(5, 4))\n",
    "contour = plt.contour(X, Y, Z, levels=20, cmap=\"viridis\")\n",
    "plt.colorbar(contour, label=\"Distance from Origin (SVD space)\")\n",
    "plt.scatter(svd_gdp[:, 0], svd_gdp[:, 1], alpha=0.7, label=\"gdp2\")\n",
    "plt.scatter(svd_svd[:, 0], svd_svd[:, 1], alpha=0.3, label=\"svd\")\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel(\"First Principal Component\")\n",
    "plt.ylabel(\"Second Principal Component\")\n",
    "plt.title(\"Truncated SVD of Weight Updates with Distance Contours\")\n",
    "\n",
    "# Add index to each point\n",
    "for i, (x, y) in enumerate(svd_gdp):\n",
    "    if i % 50 == 0:\n",
    "        plt.annotate(str(i), (x, y), xytext=(5, 5), textcoords=\"offset points\")\n",
    "\n",
    "for i, (x, y) in enumerate(svd_svd):\n",
    "    if i % 50 == 0:\n",
    "        plt.annotate(str(i), (x, y), xytext=(5, 5), textcoords=\"offset points\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDP batch-size effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prog_scheme.utils import extract_error\n",
    "from src.utils.logging_utils import LogCapture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size_ in [1, 5, 10, 20, 50, input_size]:\n",
    "    with LogCapture() as logc:\n",
    "        atile.tile.set_weights(w.T)\n",
    "        atile.program_weights(batch_size=batch_size_)\n",
    "        log_list = logc.get_log_list()\n",
    "    err_list = extract_error(log_list)\n",
    "    num_iter = len(err_list)\n",
    "    plt.semilogy(err_list, label=f\"batch_size={batch_size_}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Nuclear norm of weight error\")\n",
    "plt.title(\n",
    "    \"{}x{} rank={} matrix with {}\".format(\n",
    "        input_size, output_size, rank, atile.rpu_config.device.__class__.__name__\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d2d variaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataclass fields\n",
    "atile.rpu_config.device.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.T[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the element wise perturbation is applied\n",
    "\n",
    "atile.tile.set_weights(w.T)\n",
    "wtile = atile.tile.get_weights()\n",
    "torch.allclose(wtile, w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomTile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.tiles.custom import CustomTile\n",
    "\n",
    "ctile = CustomTile(output_size, input_size)\n",
    "ctile.get_weights(realistic=True)\n",
    "ctile2 = CustomTile(output_size, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealisticTile(Ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prog_scheme.realistic import RealisticTile, RPUConfigwithProgram\n",
    "\n",
    "# rpu_config = RPUConfigwithProgram(program_weights=gdp2)\n",
    "# ctile = RealisticTile(output_size, input_size, rpu_config=rpu_config)\n",
    "\n",
    "# rpu_config2 = RPUConfigwithProgram(program_weights=svd)\n",
    "# ctile2 = RealisticTile(output_size, input_size, rpu_config=rpu_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    ctile.set_weights(w, realistic=True)\n",
    "    log_list = logc.get_log_list()\n",
    "\n",
    "with LogCapture() as logc:\n",
    "    ctile2.set_weights(w, realistic=True)\n",
    "    log_list2 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract error and plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "err_list = extract_error(log_list)\n",
    "err_list2 = extract_error(log_list2)\n",
    "\n",
    "plt.plot(err_list, label=\"gpc\")\n",
    "plt.plot(err_list2, label=\"svd\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only `AnalogTile` which inherits `TileWithPeriphery` class has `program_weights` method\n",
    "\n",
    "`program_weights` method implements \"Gradient descent-based programming of analog in-memory computing cores\" by default\n",
    "\n",
    "`set_weights` method is used to set the weights of the analog tile to the given values\\\n",
    "`program_weights` method is internally called by `set_weights` method to program the weights of the analog tile\\\n",
    "\n",
    "`get_weights` method is used to get the weights of the analog tile\\\n",
    "`read_weights` method is used to read the weights of the analog tile with read noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.nn import AnalogLinear\n",
    "from aihwkit.optim import AnalogSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_layer = torch.nn.Linear(input_size, output_size, bias=False)\n",
    "layer = AnalogLinear.from_digital(digital_layer, rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AnalogSGD(layer.parameters(), lr=0.005)\n",
    "losses = []\n",
    "for _ in range(1000):\n",
    "    x = torch.rand(input_size)\n",
    "    yhat = layer(x)\n",
    "    loss = (yhat**2).sum()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NMDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
