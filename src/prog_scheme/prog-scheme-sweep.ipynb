{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import wandb\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    InputRangeParameter,\n",
    "    PrePostProcessingParameter,\n",
    "    UpdateParameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, eye\n",
    "from torch.autograd import no_grad\n",
    "from torch.linalg import lstsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.configs import FloatingPointRPUConfig, SingleRPUConfig\n",
    "from aihwkit.simulator.configs.devices import (\n",
    "    ConstantStepDevice,\n",
    "    DriftParameter,\n",
    "    ExpStepDevice,\n",
    "    FloatingPointDevice,\n",
    "    IdealDevice,\n",
    "    LinearStepDevice,\n",
    "    SimpleDriftParameter,\n",
    ")\n",
    "from aihwkit.simulator.parameters.enums import PulseType\n",
    "from aihwkit.simulator.presets.configs import IdealizedPreset, PCMPreset, ReRamSBPreset\n",
    "from aihwkit.simulator.presets.devices import IdealizedPresetDevice\n",
    "from aihwkit.simulator.tiles import FloatingPointTile\n",
    "from aihwkit.simulator.tiles.analog import AnalogTile\n",
    "from aihwkit.simulator.tiles.periphery import TileWithPeriphery\n",
    "\n",
    "from src.prog_scheme.kalman import ExpDeviceEKF, LinearDeviceEKF\n",
    "from src.prog_scheme.program_methods import gdp2, svd, svd_ekf_lqg, svd_kf\n",
    "from src.prog_scheme.utils import generate_target_weights, program_n_log, rpuconf2dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define customread_tile class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customread_tile(AnalogTile):\n",
    "    @no_grad()\n",
    "    def read_weights_(\n",
    "        self,\n",
    "        apply_weight_scaling: bool = False,\n",
    "        x_values: Tensor | None = None,\n",
    "        x_rand: bool = False,\n",
    "        over_sampling: int = 10,\n",
    "    ) -> tuple[Tensor, Tensor | None]:\n",
    "        \"\"\"Reads the weights (and biases) in a realistic manner\n",
    "        by using the forward pass for weights readout.\n",
    "\n",
    "        Gets the tile weights and extracts the mathematical weight\n",
    "        matrix and biases (if present, by determined by the ``self.analog_bias``\n",
    "        parameter).\n",
    "\n",
    "        The weight will not be directly read, but linearly estimated\n",
    "        using random inputs using the analog forward pass.\n",
    "\n",
    "        Note:\n",
    "            If the tile includes digital periphery (e.g. out scaling),\n",
    "            these will be applied. Thus this weight is the logical\n",
    "            weights that correspond to the weights in an FP network.\n",
    "\n",
    "        Note:\n",
    "            weights are estimated using the ``lstsq`` solver from torch.\n",
    "\n",
    "        Args:\n",
    "            apply_weight_scaling: Whether to rescale the given weight matrix\n",
    "                and populate the digital output scaling factors as\n",
    "                specified in the configuration\n",
    "                :class:`~aihwkit.simulator.configs.MappingParameter`. A\n",
    "                new ``weight_scaling_omega`` can be given. Note that\n",
    "                this will overwrite the existing digital out scaling\n",
    "                factors.\n",
    "\n",
    "            x_values: Values to use for estimating the matrix. If\n",
    "                not given, inputs are standard normal vectors.\n",
    "\n",
    "            over_sampling: If ``x_values`` is not given,\n",
    "                ``over_sampling * in_size`` random vectors are used\n",
    "                for the estimation\n",
    "\n",
    "        Returns:\n",
    "            a tuple where the first item is the ``[out_size, in_size]`` weight\n",
    "            matrix; and the second item is either the ``[out_size]`` bias vector\n",
    "            or ``None`` if the tile is set not to use bias.\n",
    "\n",
    "        Raises:\n",
    "            TileError: in case wrong code usage of TileWithPeriphery\n",
    "\n",
    "        \"\"\"\n",
    "        dtype = self.get_dtype()\n",
    "        if x_values is None:\n",
    "            x_values = eye(self.in_size, self.in_size, device=self.device, dtype=dtype)\n",
    "            if x_rand:\n",
    "                x_values = torch.rand(self.in_size, self.in_size, device=self.device, dtype=dtype)\n",
    "        else:\n",
    "            x_values = x_values.to(self.device)\n",
    "\n",
    "        x_values = x_values.repeat(over_sampling, 1)\n",
    "\n",
    "        # forward pass in eval mode\n",
    "        was_training = self.training\n",
    "        is_indexed = self.is_indexed()\n",
    "        self.eval()\n",
    "        if is_indexed:\n",
    "            self.analog_ctx.set_indexed(False)\n",
    "        y_values = self.forward(x_values)\n",
    "        if was_training:\n",
    "            self.train()\n",
    "        if is_indexed:\n",
    "            self.analog_ctx.set_indexed(True)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            y_values -= self.bias\n",
    "\n",
    "        est_weight = lstsq(x_values, y_values).solution.T.cpu()\n",
    "        weight, bias = self._separate_weights(est_weight)\n",
    "\n",
    "        if self.digital_bias:\n",
    "            bias = self.bias.detach().cpu()\n",
    "\n",
    "        if not apply_weight_scaling:\n",
    "            # we de-apply all scales\n",
    "            alpha = self.get_scales()\n",
    "            if alpha is not None:\n",
    "                alpha = alpha.detach().cpu()\n",
    "                return (weight / alpha.view(-1, 1), bias / alpha if self.analog_bias else bias)\n",
    "        return weight, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sweep main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define default parameters\n",
    "    default_config = {\n",
    "        \"input_size\": 100,\n",
    "        \"output_size\": 50,\n",
    "        \"rank\": 50,\n",
    "        \"over_sampling\": 10,\n",
    "        \"x_rand\": False,\n",
    "        \"batch_size\": 1,\n",
    "        \"tol\": 1e-8,\n",
    "        \"max_iter\": 1000,\n",
    "        \"norm_type\": \"fro\",\n",
    "        \"svd_every_k_iter\": 5,\n",
    "        \"read_noise_std\": 0.1,\n",
    "        \"update_noise_std\": 0.1,\n",
    "        \"w_init\": 0.01,\n",
    "        \"gamma_up\": 0.1,\n",
    "        \"gamma_down\": 0.1,\n",
    "        \"desired_bl\": 127,\n",
    "        \"w_max\": 1,\n",
    "        \"w_min\": -1,\n",
    "    }\n",
    "\n",
    "    # Initialize wandb\n",
    "    wandb.init(config=default_config)\n",
    "    config = wandb.config\n",
    "\n",
    "    # Extract parameters from wandb.config\n",
    "    input_size = config.input_size\n",
    "    output_size = config.output_size\n",
    "    rank = config.rank\n",
    "    dim = input_size * output_size\n",
    "    over_sampling = config.over_sampling\n",
    "    x_rand = config.x_rand\n",
    "    batch_size = config.batch_size\n",
    "    tol = config.tol\n",
    "    max_iter = config.max_iter\n",
    "    norm_type = config.norm_type\n",
    "    svd_every_k_iter = config.svd_every_k_iter\n",
    "    read_noise_std = config.read_noise_std\n",
    "    update_noise_std = config.update_noise_std\n",
    "    w_init = config.w_init\n",
    "\n",
    "    # Generate low rank matrix\n",
    "    w_target = generate_target_weights(input_size, output_size, rank)\n",
    "\n",
    "    # Configure device and RPU\n",
    "    pre_post_cfg = PrePostProcessingParameter(input_range=InputRangeParameter(enable=False))\n",
    "    device_cfg = LinearStepDevice()\n",
    "    update_cfg = UpdateParameters(pulse_type=PulseType.STOCHASTIC, desired_bl=config.desired_bl)\n",
    "    rpuconfig = SingleRPUConfig(update=update_cfg, device=device_cfg)\n",
    "    rpuconfig.forward.out_noise = read_noise_std\n",
    "    rpuconfig.device.write_noise_std = update_noise_std\n",
    "    rpuconfig.device.w_max = config.w_max\n",
    "    rpuconfig.device.gamma_up = config.gamma_up\n",
    "    rpuconfig.device.gamma_down = config.gamma_down\n",
    "    rpuconfig.device.w_min = config.w_min\n",
    "    rpuconfig.device.w_max_dtod = 0.01\n",
    "    rpuconfig.device.w_min_dtod = 0.01\n",
    "    rpuconfig.device.dw_min_std = 0.0\n",
    "    rpuconfig.device.mult_noise = False  # Additive noise\n",
    "\n",
    "    # Convert RPU config to dict\n",
    "    rpuconf_dict = rpuconf2dict(rpuconfig, max_depth=2)\n",
    "    if isinstance(rpuconfig.device, LinearStepDevice):\n",
    "        device_ekf = LinearDeviceEKF(\n",
    "            dim=dim,\n",
    "            read_noise_std=read_noise_std,\n",
    "            update_noise_std=update_noise_std,\n",
    "            **rpuconf_dict[\"device\"],\n",
    "        )\n",
    "    elif isinstance(rpuconfig.device, ExpStepDevice):\n",
    "        device_ekf = ExpDeviceEKF(\n",
    "            dim=dim,\n",
    "            read_noise_std=read_noise_std,\n",
    "            update_noise_std=update_noise_std,\n",
    "            **rpuconf_dict[\"device\"],\n",
    "        )\n",
    "    else:\n",
    "        device_ekf = None\n",
    "\n",
    "    conf = {\n",
    "        **rpuconf_dict,\n",
    "        \"matrix\": {\"input_size\": input_size, \"output_size\": output_size, \"rank\": rank},\n",
    "        \"methods\": {\n",
    "            \"device_ekf\": device_ekf,\n",
    "            \"tolerance\": tol,\n",
    "            \"max_iter\": max_iter,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"norm_type\": norm_type,\n",
    "            \"svd_every_k_iter\": svd_every_k_iter,\n",
    "            \"read_noise_std\": read_noise_std,\n",
    "            \"update_noise_std\": update_noise_std,\n",
    "            \"w_init\": w_init,\n",
    "            \"over_sampling\": over_sampling,\n",
    "            \"x_rand\": x_rand,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Initialize tiles\n",
    "    atile = customread_tile(output_size, input_size, rpu_config=rpuconfig)  # with periphery\n",
    "    atile_dic = {}\n",
    "    atile.state_dict(atile_dic)\n",
    "    # atile2 = AnalogTile(output_size, input_size, rpu_config=rpuconfig)\n",
    "    atile2 = copy.deepcopy(atile)\n",
    "    # atile2.load_state_dict(atile_dic, assign=True)\n",
    "    # atile3 = AnalogTile(output_size, input_size, rpu_config=rpuconfig)\n",
    "    atile3 = copy.deepcopy(atile)\n",
    "    atile4 = copy.deepcopy(atile)\n",
    "\n",
    "    atile.program_weights = gdp2.__get__(atile, TileWithPeriphery)\n",
    "    atile2.program_weights = svd.__get__(atile2, TileWithPeriphery)\n",
    "    atile3.program_weights = svd_kf.__get__(atile3, TileWithPeriphery)\n",
    "    atile4.program_weights = svd_ekf_lqg.__get__(atile4, TileWithPeriphery)\n",
    "    tiles = [atile, atile2, atile3]\n",
    "    tiles.append(atile4) if device_ekf is not None else None\n",
    "    method_names = [t.program_weights.__name__ for t in tiles]\n",
    "\n",
    "    # Log errors to wandb\n",
    "    for idx, tile in enumerate(tiles):\n",
    "        err = program_n_log(tile, w_target.T, **conf)\n",
    "        # Log the last error value for each method\n",
    "        wandb.log({f\"{method_names[idx]} Error\": err[-1]})\n",
    "        plt.semilogy(err, label=method_names[idx])\n",
    "\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(f\"{norm_type} norm of weight error\")\n",
    "    plt.title(f\"Error vs Iteration @ {input_size}x{output_size}, rank={rank}\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"error_plot.png\")\n",
    "    wandb.log({\"Error Plot\": wandb.Image(\"error_plot.png\")})\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sweep config 설정\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",  # 랜덤 서치를 위해서는 'random' 사용\n",
    "    \"parameters\": {\n",
    "        \"rank\": {\"values\": [10, 20, 30, 40, 50]},\n",
    "        \"svd_every_k_iter\": {\"values\": [1, 5, 10]},\n",
    "    },\n",
    "}\n",
    "\n",
    "# Sweep 실행\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"prog-scheme-sweep\")\n",
    "wandb.agent(sweep_id, function=main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prog_scheme_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
