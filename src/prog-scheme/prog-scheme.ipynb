{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primitive update method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from aihwkit.simulator.tiles.custom import CustomSimulatorTile\n",
    "from torch import Tensor\n",
    "\n",
    "# TODO: 아래 메서드들 구현 필요\n",
    "# aihwkit의 PulseType(aihwkit.simulator.parameters.enums.PulseType)들과 연동해서 구현.\n",
    "# PCM inference noise 모델도 추가해보기\n",
    "\n",
    "# Note: SimulaterTile.forward() 말고 AnalogNVM의 matmul에다가 구현해도 됨\n",
    "# AnalogMVM.matmul()\n",
    "# -> SimulatorTile.forward()\n",
    "# -> PeripheryTile.joint_forward()\n",
    "# -> AnalogFunction.forward()\n",
    "# -> CustomTile.forward()\n",
    "# -> PeripheryTile.read_weights()/program_weights()\n",
    "# -> PeripheryTile.get_weights/set_weights(realistic=True) 에 쓰임\n",
    "\n",
    "\n",
    "class _HalfSelectMixin:\n",
    "    \"\"\"Implements the half-selected update method.\"\"\"\n",
    "\n",
    "    def half_selection(self): ...\n",
    "\n",
    "\n",
    "class HalfSelectedSimulatorTile(_HalfSelectMixin, CustomSimulatorTile):\n",
    "\n",
    "    def set_weights(self, weight: Tensor) -> None:\n",
    "        \"\"\"Set the tile weights.\n",
    "\n",
    "        Args:\n",
    "            weight: ``[out_size, in_size]`` weight matrix.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"set_weights() is not implemented\")\n",
    "\n",
    "    def get_weights(self) -> Tensor:\n",
    "        \"\"\"Get the tile weights.\n",
    "\n",
    "        Returns:\n",
    "            a tuple where the first item is the ``[out_size, in_size]`` weight\n",
    "            matrix; and the second item is either the ``[out_size]`` bias vector\n",
    "            or ``None`` if the tile is set not to use bias.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"get_weights() is not implemented\")\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        x_input: Tensor,\n",
    "        d_input: Tensor,\n",
    "        bias: bool = False,\n",
    "        in_trans: bool = False,\n",
    "        out_trans: bool = False,\n",
    "        non_blocking: bool = False,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Implements rank-1 tile update with gradient noise\n",
    "        (e.g. using pulse trains).\n",
    "\n",
    "        Note:\n",
    "            Ignores additional arguments\n",
    "\n",
    "        Raises:\n",
    "            TileError: in case transposed input / output or bias is requested\n",
    "        \"\"\"\n",
    "        super().update(x_input, d_input, bias, in_trans, out_trans, non_blocking)\n",
    "        self.half_selection()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_input: Tensor,\n",
    "        bias: bool = False,\n",
    "        in_trans: bool = False,\n",
    "        out_trans: bool = False,\n",
    "        is_test: bool = False,\n",
    "        non_blocking: bool = False,\n",
    "    ) -> Tensor:\n",
    "        return super().forward(x_input, bias, in_trans, out_trans, is_test, non_blocking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomTile & RPUConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Optional, Type\n",
    "\n",
    "from aihwkit.simulator.tiles.custom import CustomRPUConfig, CustomTile\n",
    "\n",
    "\n",
    "class MyCustomTile(CustomTile):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_size: int,\n",
    "        in_size: int,\n",
    "        rpu_config: Optional[\"RPUConfigwithProgram\"],\n",
    "        bias: bool = False,\n",
    "        in_trans: bool = False,\n",
    "        out_trans: bool = False,\n",
    "    ):\n",
    "        super().__init__(out_size, in_size, rpu_config, bias, in_trans, out_trans)\n",
    "        # dynamically add the program_weights method\n",
    "        self.program_weights = rpu_config.program_weights.__get__(self, MyCustomTile)\n",
    "\n",
    "\n",
    "# TODO: dataclass에 직접 program_weights 메서드 붙여넣기?\n",
    "@dataclass\n",
    "class RPUConfigwithProgram(CustomRPUConfig):\n",
    "    \"\"\"Custom single RPU configuration.\"\"\"\n",
    "\n",
    "    program_weights: Callable[[Any], None] = None\n",
    "    \"\"\"Method to program the weights.\"\"\"\n",
    "\n",
    "    tile_class: Type = MyCustomTile\n",
    "    \"\"\"Tile class that corresponds to this RPUConfig.\"\"\"\n",
    "\n",
    "    simulator_tile_class: Type = HalfSelectedSimulatorTile\n",
    "    \"\"\"Simulator tile class implementing the analog forward / backward / update.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient-based programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from src.utils.pylogger import RankedLogger\n",
    "\n",
    "log = RankedLogger()\n",
    "\n",
    "# TODO: realistic한가?\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_gpc(\n",
    "    self,\n",
    "    from_reference: bool = True,\n",
    "    x_values: Optional[Tensor] = None,\n",
    "    learning_rate: float = 0.1,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.01,\n",
    ") -> None:\n",
    "    \"\"\"Programm the target weights into the conductances using the\n",
    "    pulse update defined.\n",
    "\n",
    "    Programming is done using the defined tile-update (e.g. SGD)\n",
    "    and matching inputs (`x_values` by default `eye`).\n",
    "\n",
    "    Args:\n",
    "\n",
    "        from_reference: Whether to use weights from reference\n",
    "            (those that were initally set with `set_weights`) or\n",
    "            the current weights.\n",
    "        x_values: Values to use for the read-and verify. If none\n",
    "            are given, unit-vectors are used\n",
    "        learning_rate: Learning rate of the optimization\n",
    "        max_iter: max number of batches for the iterative programming\n",
    "        tolerance: Stop the iteration loop early if the mean\n",
    "            output deviation is below this number. Given in\n",
    "            relation to the max output.\n",
    "        w_init: initial weight matrix to start from. If given as\n",
    "            float, weights are set uniform random in `[-w_init,\n",
    "            w_init]`. This init weight is given directly in\n",
    "            normalized conductance units and should include the\n",
    "            bias row if existing.\n",
    "    \"\"\"\n",
    "\n",
    "    if not from_reference or self.reference_combined_weights is None:\n",
    "        self.reference_combined_weights = self.tile.get_weights()\n",
    "        target_weights = self.reference_combined_weights\n",
    "\n",
    "    if x_values is None:\n",
    "        x_values = torch.eye(self.tile.get_x_size())\n",
    "        x_values = x_values.to(self.device)\n",
    "        target_values = x_values @ target_weights.to(self.device).T\n",
    "\n",
    "    target_max = target_values.abs().max().item()\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    self.tile.set_learning_rate(learning_rate)  # type: ignore\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        y = self.tile.forward(x_values, False)\n",
    "        # TODO: error와 weight 2norm 사이 관계 분석\n",
    "        error = y - target_values\n",
    "        err_normalized = error.abs().mean().item() / target_max\n",
    "        log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and err_normalized < tolerance:\n",
    "            break\n",
    "        self.tile.update(x_values, error, False)  # type: ignore\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed method(SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compensate_half_selection(v: Tensor) -> Tensor:\n",
    "    \"\"\"Compensate the half-selection effect for a vector.\n",
    "\n",
    "    Args:\n",
    "        v: Vector to compensate.\n",
    "\n",
    "    Returns:\n",
    "        Compensated vector.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_svd(\n",
    "    self,\n",
    "    from_reference: bool = True,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.01,\n",
    "    rank_atol: float = 1e-6,\n",
    "    rank_rtol: float = 1e-6,\n",
    "    svd_once: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform singular value decomposition (SVD) based weight programming.\n",
    "\n",
    "    Args:\n",
    "        from_reference (bool, optional): Flag indicating whether to use reference combined weights. Defaults to True.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 100.\n",
    "        tolerance (float, optional): Tolerance for convergence. Defaults to 0.01.\n",
    "        w_init (Union[float, Tensor], optional): Initial value for weights. Defaults to 0.01.\n",
    "        rank_atol (float, optional): Absolute tolerance for numerical rank computation. Defaults to 1e-6.\n",
    "        rank_rtol (float, optional): Relative tolerance for numerical rank computation. Defaults to 1e-6.\n",
    "        svd_once (bool, optional): Flag indicating whether to perform SVD only once. Defaults to False.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if not from_reference or self.reference_combined_weights is None:\n",
    "        self.reference_combined_weights = self.tile.get_weights()\n",
    "        target_weights = self.reference_combined_weights\n",
    "\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    self.tile.set_learning_rate(1)  # type: ignore\n",
    "    U, S, Vh = torch.linalg.svd(target_weights - self.tile.get_weights())\n",
    "    # compute numerical rank from S given rtol, atol\n",
    "    num_rank = torch.sum(S > torch.max(rank_atol, S * rank_rtol))\n",
    "    assert num_rank == torch.linalg.matrix_rank(\n",
    "        target_weights - self.tile.get_weights(), rank_atol, rank_rtol\n",
    "    )\n",
    "    i = 0\n",
    "    for _ in range(min(num_rank, max_iter)):\n",
    "        u1 = U[i] * torch.sqrt(S[i])\n",
    "        v1 = Vh[i] * torch.sqrt(S[i])\n",
    "        u1, v1 = compensate_half_selection(u1), compensate_half_selection(v1)\n",
    "        self.tile.update(u1, v1, False)\n",
    "        diff = target_weights - self.get_weights(realistic=True)\n",
    "        l2_norm = diff.norm()\n",
    "        log.debug(f\"Error: {l2_norm}\")\n",
    "        if tolerance is not None and l2_norm < tolerance:\n",
    "            break\n",
    "        elif svd_once:\n",
    "            i += 1\n",
    "        else:\n",
    "            U, S, Vh = torch.linalg.svd(diff)\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnalogTile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.configs import SingleRPUConfig\n",
    "from aihwkit.simulator.configs.devices import ConstantStepDevice, DriftParameter\n",
    "from aihwkit.simulator.configs.utils import InputRangeParameter, PrePostProcessingParameter\n",
    "from aihwkit.simulator.tiles.analog import AnalogTile\n",
    "\n",
    "input_size = 6\n",
    "output_size = 3\n",
    "pre_post = PrePostProcessingParameter(input_range=InputRangeParameter(enable=True))\n",
    "device = ConstantStepDevice(diffusion=0, drift=DriftParameter())\n",
    "rpuconfig = SingleRPUConfig(device=device, pre_post=pre_post)\n",
    "atile = AnalogTile(output_size, input_size, rpu_config=rpuconfig)  # with periphery\n",
    "print(rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atile.program_weights = program_weights_gpc.__get__(atile, AnalogTile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.utils.logging_utils import LogCapture\n",
    "\n",
    "w = torch.rand_like(atile.get_weights()[0])\n",
    "\n",
    "with LogCapture() as logc:\n",
    "    atile.set_weights(weight=w, realistic=True)\n",
    "    log_list = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.utils.logging_utils import LogCapture\n",
    "\n",
    "w = torch.rand_like(atile.get_weights()[0])\n",
    "\n",
    "with LogCapture() as logc:\n",
    "    atile.set_weights(weight=w, realistic=True)\n",
    "    log_list = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form log list[string], search Error: {err_normalized} pattern and extract the value\n",
    "# into list\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_error(log_list):\n",
    "    err_list = []\n",
    "\n",
    "    for log in log_list:\n",
    "        if \"Error\" in log:\n",
    "            err_list.append(float(re.findall(r\"Error: ([0-9.]+)\", log)[0]))\n",
    "\n",
    "    return err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "err_list = extract_error(log_list)\n",
    "plt.plot(err_list)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPC vs SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpu_config = RPUConfigwithProgram(program_weights=program_weights_gpc)\n",
    "ctile = MyCustomTile(output_size, input_size, rpu_config=rpu_config)\n",
    "\n",
    "rpu_config2 = RPUConfigwithProgram(program_weights=program_weights_svd)\n",
    "ctile2 = MyCustomTile(output_size, input_size, rpu_config=rpu_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    ctile.set_weights(realistic=True)\n",
    "    log_list = logc.get_log_list()\n",
    "\n",
    "with LogCapture() as logc:\n",
    "    ctile2.set_weights()\n",
    "    log_list2 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract error and plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "err_list = extract_error(log_list)\n",
    "err_list2 = extract_error(log_list2)\n",
    "\n",
    "plt.plot(err_list, label=\"gpc\")\n",
    "plt.plot(err_list2, label=\"svd\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only `AnalogTile` which inherits `TileWithPeriphery` class has `program_weights` method\n",
    "\n",
    "`program_weights` method implements \"Gradient descent-based programming of analog in-memory computing cores\" by default\n",
    "\n",
    "`set_weights` method is used to set the weights of the analog tile to the given values\\\n",
    "`program_weights` method is internally called by `set_weights` method to program the weights of the analog tile\\\n",
    "\n",
    "`get_weights` method is used to get the weights of the analog tile\\\n",
    "`read_weights` method is used to read the weights of the analog tile with read noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.nn import AnalogLinear\n",
    "from aihwkit.optim import AnalogSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_layer = torch.nn.Linear(input_size, output_size, bias=False)\n",
    "layer = AnalogLinear.from_digital(digital_layer, rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AnalogSGD(layer.parameters(), lr=0.005)\n",
    "losses = []\n",
    "for _ in range(1000):\n",
    "    x = torch.rand(input_size)\n",
    "    yhat = layer(x)\n",
    "    loss = (yhat**2).sum()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
