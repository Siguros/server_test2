{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primitive update method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from aihwkit.exceptions import TorchTileConfigError\n",
    "from aihwkit.simulator.configs import TorchInferenceRPUConfig\n",
    "from aihwkit.simulator.parameters.enums import WeightClipType\n",
    "from aihwkit.simulator.parameters.inference import WeightClipParameter, WeightModifierParameter\n",
    "from aihwkit.simulator.tiles.custom import CustomSimulatorTile\n",
    "from torch import Tensor\n",
    "\n",
    "# TODO: 아래 메서드들 구현 필요\n",
    "# aihwkit의 PulseType(aihwkit.simulator.parameters.enums.PulseType)들과 연동해서 구현.\n",
    "# PCM inference noise 모델 등등 추가해보기\n",
    "\n",
    "# Note: SimulaterTile.forward() 말고 AnalogNVM의 matmul에다가 구현해도 됨\n",
    "# AnalogMVM.matmul()\n",
    "# -> SimulatorTile.forward()\n",
    "# -> PeripheryTile.joint_forward()\n",
    "# -> AnalogFunction.forward()\n",
    "# -> CustomTile.forward()\n",
    "# -> PeripheryTile.read_weights()/program_weights()\n",
    "# -> PeripheryTile.get_weights/set_weights(realistic=True) 에 쓰임\n",
    "\n",
    "\n",
    "class _HalfSelectMixin:\n",
    "    \"\"\"Implements the half-selected update method.\"\"\"\n",
    "\n",
    "    def half_selection(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class HalfSelectedSimulatorTile(_HalfSelectMixin, CustomSimulatorTile):\n",
    "\n",
    "    def set_weights(self, weight: Tensor, **kwargs) -> None:\n",
    "        \"\"\"Set the tile weights.\n",
    "\n",
    "        Args:\n",
    "            weight: ``[out_size, in_size]`` weight matrix.\n",
    "        \"\"\"\n",
    "        super().set_weights(weight, **kwargs)\n",
    "\n",
    "    def get_weights(self) -> Tensor:\n",
    "        \"\"\"Get the tile weights.\n",
    "\n",
    "        Returns:\n",
    "            a tuple where the first item is the ``[out_size, in_size]`` weight\n",
    "            matrix; and the second item is either the ``[out_size]`` bias vector\n",
    "            or ``None`` if the tile is set not to use bias.\n",
    "        \"\"\"\n",
    "        super().get_weights()\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        x_input: Tensor,\n",
    "        d_input: Tensor,\n",
    "        bias: bool = False,\n",
    "        in_trans: bool = False,\n",
    "        out_trans: bool = False,\n",
    "        non_blocking: bool = False,\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Implements rank-1 tile update with gradient noise\n",
    "        (e.g. using pulse trains).\n",
    "\n",
    "        Note:\n",
    "            Ignores additional arguments\n",
    "\n",
    "        Raises:\n",
    "            TileError: in case transposed input / output or bias is requested\n",
    "        \"\"\"\n",
    "        super().update(x_input, d_input, bias, in_trans, out_trans, non_blocking)\n",
    "        self.half_selection()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_input: Tensor,\n",
    "        bias: bool = False,\n",
    "        in_trans: bool = False,\n",
    "        out_trans: bool = False,\n",
    "        is_test: bool = False,\n",
    "        non_blocking: bool = False,\n",
    "    ) -> Tensor:\n",
    "        if not is_test:\n",
    "            noisy_weights = HalfSelectedSimulatorTile.modify_weight(\n",
    "                self.weight, self._modifier, x_input.shape[0]\n",
    "            )\n",
    "        else:\n",
    "            noisy_weights = self.weight\n",
    "\n",
    "        ...\n",
    "\n",
    "    @staticmethod\n",
    "    def modify_weight(inp_weight: Tensor, modifier: WeightModifierParameter, batch_size: int):\n",
    "        pass\n",
    "\n",
    "    def set_config(self, rpu_config: \"TorchInferenceRPUConfig\") -> None:\n",
    "        \"\"\"Updated the configuration to allow on-the-fly changes.\n",
    "\n",
    "        Args:\n",
    "            rpu_config: configuration to use in the next forward passes.\n",
    "        \"\"\"\n",
    "        self._f_io = rpu_config.forward\n",
    "        self._modifier = rpu_config.modifier\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def clip_weights(self, clip: WeightClipParameter) -> None:\n",
    "        \"\"\"Clip the weights. Called by InferenceTileWithperiphery.post_update_step()\n",
    "\n",
    "        Args:\n",
    "            clip: parameters specifying the clipping methof and type.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: For unsupported WeightClipTypes\n",
    "            ConfigError: If unknown WeightClipType used.\n",
    "        \"\"\"\n",
    "\n",
    "        if clip.type == WeightClipType.FIXED_VALUE:\n",
    "            self.weight.data = torch.clamp(self.weight, -clip.fixed_value, clip.fixed_value)\n",
    "        elif clip.type == WeightClipType.LAYER_GAUSSIAN:\n",
    "            alpha = self.weight.std() * clip.sigma\n",
    "            if clip.fixed_value > 0:\n",
    "                alpha = min(clip.fixed_value, alpha)\n",
    "            self.weight.data = torch.clamp(self.weight, -alpha, alpha)\n",
    "\n",
    "        elif clip.type == WeightClipType.AVERAGE_CHANNEL_MAX:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise TorchTileConfigError(f\"Unknown clip type {clip.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomTile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Tuple, Type\n",
    "\n",
    "from aihwkit.exceptions import AnalogBiasConfigError, TileError\n",
    "from aihwkit.simulator.tiles.base import SimulatorTile, SimulatorTileWrapper\n",
    "from aihwkit.simulator.tiles.custom import CustomRPUConfig\n",
    "from aihwkit.simulator.tiles.functions import AnalogFunction\n",
    "from aihwkit.simulator.tiles.inference import InferenceTileWithPeriphery\n",
    "from aihwkit.simulator.tiles.module import TileModule\n",
    "\n",
    "\n",
    "class RealisticTile(TileModule, InferenceTileWithPeriphery, SimulatorTileWrapper):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Note) methods in below are from CustomTile\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_size: int,\n",
    "        in_size: int,\n",
    "        rpu_config: Optional[\"RPUConfigwithProgram\"],\n",
    "        bias: bool = False,\n",
    "        in_trans: bool = False,\n",
    "        out_trans: bool = False,\n",
    "    ):\n",
    "        if in_trans or out_trans:\n",
    "            raise TileError(\"in/out trans is not supported.\")\n",
    "\n",
    "        if not rpu_config:\n",
    "            rpu_config = CustomRPUConfig()\n",
    "\n",
    "        TileModule.__init__(self)\n",
    "        SimulatorTileWrapper.__init__(\n",
    "            self,\n",
    "            out_size,\n",
    "            in_size,\n",
    "            rpu_config,  # type: ignore\n",
    "            bias,\n",
    "            in_trans,\n",
    "            out_trans,\n",
    "            torch_update=True,\n",
    "        )\n",
    "        InferenceTileWithPeriphery.__init__(self)\n",
    "\n",
    "        if self.analog_bias:\n",
    "            raise AnalogBiasConfigError(\"Analog bias is not supported for the torch tile\")\n",
    "        # dynamically add the program_weights method\n",
    "        self.program_weights = rpu_config.program_weights.__get__(self, RealisticTile)\n",
    "\n",
    "    def _create_simulator_tile(  # type: ignore\n",
    "        self, x_size: int, d_size: int, rpu_config: \"CustomRPUConfig\"\n",
    "    ) -> \"SimulatorTile\":\n",
    "        \"\"\"Create a simulator tile.\n",
    "\n",
    "        Args:\n",
    "            weight: 2D weight\n",
    "            rpu_config: resistive processing unit configuration\n",
    "\n",
    "        Returns:\n",
    "            a simulator tile based on the specified configuration.\n",
    "        \"\"\"\n",
    "        return rpu_config.simulator_tile_class(x_size=x_size, d_size=d_size, rpu_config=rpu_config)\n",
    "\n",
    "    def forward(\n",
    "        self, x_input: Tensor, tensor_view: Optional[Tuple] = None  # type: ignore\n",
    "    ) -> Tensor:\n",
    "        \"\"\"Torch forward function that calls the analog context forward\"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "\n",
    "        # to enable on-the-fly changes. However, with caution: might\n",
    "        # change rpu config for backward / update while doing another forward.\n",
    "        self.tile.set_config(self.rpu_config)\n",
    "\n",
    "        out = AnalogFunction.apply(\n",
    "            self.get_analog_ctx(), self, x_input, self.shared_weights, not self.training\n",
    "        )\n",
    "\n",
    "        if tensor_view is None:\n",
    "            tensor_view = self.get_tensor_view(out.dim())\n",
    "        out = self.apply_out_scaling(out, tensor_view)\n",
    "\n",
    "        if self.digital_bias:\n",
    "            return out + self.bias.view(*tensor_view)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPUConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: dataclass에 직접 program_weights 메서드 붙여넣기?\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RPUConfigwithProgram(CustomRPUConfig):\n",
    "    \"\"\"Custom single RPU configuration.\"\"\"\n",
    "\n",
    "    program_weights: Callable[[Any], None] = None\n",
    "    \"\"\"Method to program the weights.\"\"\"\n",
    "\n",
    "    tile_class: Type = RealisticTile\n",
    "    \"\"\"Tile class that corresponds to this RPUConfig.\"\"\"\n",
    "\n",
    "    simulator_tile_class: Type = HalfSelectedSimulatorTile\n",
    "    \"\"\"Simulator tile class implementing the analog forward / backward / update.\"\"\"\n",
    "\n",
    "    clip: WeightClipParameter = field(\n",
    "        default_factory=lambda: WeightClipParameter(\n",
    "            type=WeightClipType.FIXED_VALUE, fixed_value=1.0\n",
    "        )\n",
    "    )\n",
    "    modifier: WeightModifierParameter = field(default_factory=WeightModifierParameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent-based programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Union\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "from src.utils.pylogger import RankedLogger\n",
    "\n",
    "log = RankedLogger()\n",
    "\n",
    "# TODO: realistic한가?\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_gdp(\n",
    "    self,\n",
    "    from_reference: bool = True,\n",
    "    x_values: Optional[Tensor] = None,\n",
    "    learning_rate: float = 1,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.01,\n",
    ") -> None:\n",
    "    \"\"\"Programm the target weights into the conductances using the\n",
    "    pulse update defined.\n",
    "\n",
    "    Programming is done using the defined tile-update (e.g. SGD)\n",
    "    and matching inputs (`x_values` by default `eye`).\n",
    "\n",
    "    Args:\n",
    "\n",
    "        from_reference: Whether to use weights from reference\n",
    "            (those that were initally set with `set_weights`) or\n",
    "            the current weights.\n",
    "        x_values: Values to use for the read-and verify. If none\n",
    "            are given, unit-vectors are used\n",
    "        learning_rate: Learning rate of the optimization\n",
    "        max_iter: max number of batches for the iterative programming\n",
    "        tolerance: Stop the iteration loop early if the mean\n",
    "            output deviation is below this number. Given in\n",
    "            relation to the max output.\n",
    "        w_init: initial weight matrix to start from. If given as\n",
    "            float, weights are set uniform random in `[-w_init,\n",
    "            w_init]`. This init weight is given directly in\n",
    "            normalized conductance units and should include the\n",
    "            bias row if existing.\n",
    "    \"\"\"\n",
    "\n",
    "    if not from_reference or self.reference_combined_weights is None:\n",
    "        self.reference_combined_weights = self.tile.get_weights()\n",
    "        target_weights = self.reference_combined_weights\n",
    "\n",
    "    if x_values is None:\n",
    "        x_values = torch.eye(self.tile.get_x_size())\n",
    "        # x_values = torch.rand(self.tile.get_x_size(), self.tile.get_x_size())\n",
    "        x_values = x_values.to(self.device)\n",
    "        target_values = x_values @ target_weights.to(self.device).T\n",
    "\n",
    "    target_max = target_values.abs().max().item()\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    self.tile.set_learning_rate(learning_rate)  # type: ignore\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        y = self.tile.forward(x_values, False)\n",
    "        # TODO: error와 weight 2norm 사이 관계 분석\n",
    "        error = y - target_values\n",
    "        err_normalized = error.abs().mean().item() / target_max\n",
    "        mtx_diff = self.tile.get_weights() - target_weights\n",
    "        l2_norm = torch.linalg.matrix_norm(mtx_diff, ord=2)\n",
    "        log.debug(f\"Error: {l2_norm}\")\n",
    "        # log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and err_normalized < tolerance:\n",
    "            break\n",
    "        self.tile.update(x_values, error, False)  # type: ignore\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = RankedLogger()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_gdp2(\n",
    "    self,\n",
    "    batch_size: int = 5,\n",
    "    learning_rate: float = 1,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.01,\n",
    ") -> None:\n",
    "    \"\"\"Programm the target weights into the conductances using the\n",
    "    pulse update defined.\n",
    "\n",
    "    Variable batch version of the `program_weights_gdp` method.\n",
    "    \"\"\"\n",
    "    target_weights = self.tile.get_weights()\n",
    "\n",
    "    input_size = self.tile.get_x_size()\n",
    "    x_values = torch.eye(input_size)\n",
    "    x_values = x_values.to(self.device)\n",
    "    target_values = x_values @ target_weights.to(self.device).T\n",
    "\n",
    "    target_max = target_values.abs().max().item()\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    self.tile.set_learning_rate(learning_rate)  # type: ignore\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "\n",
    "        if end_idx > len(x_values):\n",
    "            # Calculate how much we exceed the length\n",
    "            exceed_length = end_idx - len(x_values)\n",
    "\n",
    "            # Slice the arrays and concatenate the exceeded part from the beginning\n",
    "            x = torch.concatenate((x_values[start_idx:], x_values[:exceed_length]))\n",
    "            target = torch.concatenate((target_values[start_idx:], target_values[:exceed_length]))\n",
    "        else:\n",
    "            x = x_values[start_idx:end_idx]\n",
    "            target = target_values[start_idx:end_idx]\n",
    "\n",
    "        y = self.tile.forward(x, False)\n",
    "        error = y - target\n",
    "        err_normalized = error.abs().mean().item() / target_max\n",
    "        mtx_diff = self.tile.get_weights() - target_weights\n",
    "        l2_norm = torch.linalg.matrix_norm(mtx_diff, ord=2)\n",
    "        log.debug(f\"Error: {l2_norm}\")\n",
    "        # log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and err_normalized < tolerance:\n",
    "            break\n",
    "        self.tile.update(x, error, False)  # type: ignore\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed method(SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compensate_half_selection(v: Tensor) -> Tensor:\n",
    "    \"\"\"Compensate the half-selection effect for a vector.\n",
    "\n",
    "    Args:\n",
    "        v: Vector to compensate.\n",
    "\n",
    "    Returns:\n",
    "        Compensated vector.\n",
    "    \"\"\"\n",
    "    return v\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_svd(\n",
    "    self,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.0,\n",
    "    rank_atol: Optional[float] = 1e-2,\n",
    "    svd_once: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform singular value decomposition (SVD) based weight programming.\n",
    "\n",
    "    Args:\n",
    "        from_reference (bool, optional): Flag indicating whether to use reference combined weights. Defaults to True.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 100.\n",
    "        tolerance (float, optional): Tolerance for convergence. Defaults to 0.01.\n",
    "        w_init (Union[float, Tensor], optional): Initial value for weights. Defaults to 0.01.\n",
    "        rank_atol (float, optional): Absolute tolerance for numerical rank computation. Defaults to 1e-6.\n",
    "        rank_rtol (float, optional): Relative tolerance for numerical rank computation. Defaults to 1e-6.\n",
    "        svd_once (bool, optional): Flag indicating whether to perform SVD only once. Defaults to False.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    target_weights = self.tile.get_weights()\n",
    "\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    # x_values = torch.eye(self.tile.get_x_size())\n",
    "    # x_values = x_values.to(self.device)\n",
    "    # target_values = x_values @ target_weights.to(self.device).T\n",
    "    # target_max = target_values.abs().max().item()\n",
    "    self.tile.set_learning_rate(1)  # type: ignore\n",
    "    # since tile.update() updates w -= lr*delta_w so flip the sign\n",
    "    diff = self.tile.get_weights() - target_weights\n",
    "    # normalize diff matrix\n",
    "    U, S, Vh = torch.linalg.svd(diff, full_matrices=False)\n",
    "    # rank = torch.linalg.matrix_rank(diff)\n",
    "    if rank_atol is None:\n",
    "        rank_atol = S.max() * max(diff.shape) * torch.finfo(S.dtype).eps\n",
    "\n",
    "    rank = torch.sum(S > rank_atol).item()\n",
    "    i = 0\n",
    "    for _ in range(min(max_iter, rank)):\n",
    "        u = U[:, i]\n",
    "        v = Vh[i, :]\n",
    "        # uv_ratio = torch.sqrt(u/v)\n",
    "        uv_ratio = 1\n",
    "        sqrt_s = torch.sqrt(S[i])\n",
    "        v *= uv_ratio * sqrt_s\n",
    "        u *= sqrt_s / uv_ratio\n",
    "        u1, v1 = compensate_half_selection(u), compensate_half_selection(v)\n",
    "        self.tile.update(v1, u1, False)\n",
    "\n",
    "        # TODO: self.get_weights()\n",
    "        diff = self.tile.get_weights() - target_weights\n",
    "        U, S, Vh = torch.linalg.svd(diff, full_matrices=False)\n",
    "        l2_norm = torch.linalg.matrix_norm(diff, ord=2) if svd_once else S[i]\n",
    "        log.debug(f\"Error: {l2_norm}\")\n",
    "        # y = self.tile.forward(x_values, False)\n",
    "        # # TODO: error와 weight 2norm 사이 관계 분석\n",
    "        # error = y - target_values\n",
    "        # err_normalized = error.abs().mean().item() / target_max\n",
    "        # log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and l2_norm < tolerance:\n",
    "            break\n",
    "        elif svd_once:\n",
    "            i += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form log list[string], search Error: {err_normalized} pattern and extract the value\n",
    "# into list\n",
    "import re\n",
    "\n",
    "from src.utils.logging_utils import LogCapture\n",
    "\n",
    "\n",
    "def extract_error(log_list):\n",
    "    err_list = []\n",
    "\n",
    "    for log in log_list:\n",
    "        if \"Error\" in log:\n",
    "            err_list.append(float(re.findall(r\"Error: ([0-9.e-]+)\", log)[0]))\n",
    "\n",
    "    return err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 500\n",
    "output_size = 300\n",
    "batch_size = 5\n",
    "\n",
    "# generate low rank matrix\n",
    "rank = 20\n",
    "u, s, v = torch.svd(torch.randn(input_size, output_size))\n",
    "w = torch.mm(u[:, :rank], torch.mm(torch.diag(s[:rank]), v[:, :rank].t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnalogTile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.configs import SingleRPUConfig\n",
    "from aihwkit.simulator.configs.devices import ConstantStepDevice, DriftParameter, IdealDevice\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    InputRangeParameter,\n",
    "    PrePostProcessingParameter,\n",
    "    UpdateParameters,\n",
    ")\n",
    "from aihwkit.simulator.parameters.enums import PulseType\n",
    "from aihwkit.simulator.tiles.analog import AnalogTile\n",
    "\n",
    "pre_post_cfg = PrePostProcessingParameter(input_range=InputRangeParameter(enable=True))\n",
    "# device_cfg = ConstantStepDevice(diffusion=0)\n",
    "device_cfg = IdealDevice()\n",
    "update_cfg = UpdateParameters(pulse_type=PulseType.STOCHASTIC_COMPRESSED)\n",
    "rpuconfig = SingleRPUConfig(update=update_cfg, device=device_cfg, pre_post=pre_post_cfg)\n",
    "# rpuconfig = SingleRPUConfig()\n",
    "atile = AnalogTile(output_size, input_size, rpu_config=rpuconfig)  # with periphery\n",
    "atile_dic = {}\n",
    "atile.state_dict(atile_dic)\n",
    "atile2 = AnalogTile(output_size, input_size, rpu_config=rpuconfig)\n",
    "atile2.load_state_dict(atile_dic, assign=True)\n",
    "print(rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.tiles.periphery import TileWithPeriphery\n",
    "\n",
    "atile.program_weights = program_weights_gdp2.__get__(atile, TileWithPeriphery)\n",
    "atile2.program_weights = program_weights_svd.__get__(atile2, TileWithPeriphery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    atile.tile.set_weights(w.T)\n",
    "    atile.program_weights(batch_size=batch_size, tolerance=1e-5)\n",
    "    log_list1 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    # atile2.set_weights(weight=w, realistic=True)\n",
    "    atile2.tile.set_weights(w.T)\n",
    "    atile2.program_weights()\n",
    "    log_list2 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "err_list = extract_error(log_list1)\n",
    "err_list2 = extract_error(log_list2)\n",
    "plt.semilogy(err_list)\n",
    "plt.semilogy(err_list2)\n",
    "# set legend\n",
    "plt.legend([f\"gdp-seq(batchsize {batch_size})\", \"svd\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"L2 norm of weight error (largest singular value)\")\n",
    "plt.title(\"Error vs Iteration @ {}x{}, rank={}\".format(input_size, output_size, rank))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDP batch-size effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in [1, 5, 10, 20, 50]:\n",
    "    with LogCapture() as logc:\n",
    "        atile.tile.set_weights(w.T)\n",
    "        atile.program_weights(batch_size=batch_size)\n",
    "        log_list = logc.get_log_list()\n",
    "    err_list = extract_error(log_list)\n",
    "    num_iter = len(err_list)\n",
    "    plt.semilogy(err_list, label=f\"batch_size={batch_size}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"L2 norm of weight error\")\n",
    "plt.title(\n",
    "    \"{}x{} rank={} matrix with {}\".format(\n",
    "        input_size, output_size, rank, atile.rpu_config.device.__class__.__name__\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomTile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.tiles.custom import CustomTile\n",
    "\n",
    "ctile = CustomTile(output_size, input_size)\n",
    "ctile.get_weights(realistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealisticTile(Ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpu_config = RPUConfigwithProgram(program_weights=program_weights_gdp)\n",
    "ctile = RealisticTile(output_size, input_size, rpu_config=rpu_config)\n",
    "\n",
    "rpu_config2 = RPUConfigwithProgram(program_weights=program_weights_svd)\n",
    "ctile2 = RealisticTile(output_size, input_size, rpu_config=rpu_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    ctile.set_weights(w, realistic=True)\n",
    "    log_list = logc.get_log_list()\n",
    "\n",
    "with LogCapture() as logc:\n",
    "    ctile2.set_weights(w, realistic=True)\n",
    "    log_list2 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract error and plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "err_list = extract_error(log_list)\n",
    "err_list2 = extract_error(log_list2)\n",
    "\n",
    "plt.plot(err_list, label=\"gpc\")\n",
    "plt.plot(err_list2, label=\"svd\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only `AnalogTile` which inherits `TileWithPeriphery` class has `program_weights` method\n",
    "\n",
    "`program_weights` method implements \"Gradient descent-based programming of analog in-memory computing cores\" by default\n",
    "\n",
    "`set_weights` method is used to set the weights of the analog tile to the given values\\\n",
    "`program_weights` method is internally called by `set_weights` method to program the weights of the analog tile\\\n",
    "\n",
    "`get_weights` method is used to get the weights of the analog tile\\\n",
    "`read_weights` method is used to read the weights of the analog tile with read noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.nn import AnalogLinear\n",
    "from aihwkit.optim import AnalogSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_layer = torch.nn.Linear(input_size, output_size, bias=False)\n",
    "layer = AnalogLinear.from_digital(digital_layer, rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AnalogSGD(layer.parameters(), lr=0.005)\n",
    "losses = []\n",
    "for _ in range(1000):\n",
    "    x = torch.rand(input_size)\n",
    "    yhat = layer(x)\n",
    "    loss = (yhat**2).sum()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
