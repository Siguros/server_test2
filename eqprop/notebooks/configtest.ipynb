{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydra-zen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Hydra compatible cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import omegaconf\n",
    "import pyrootutils\n",
    "\n",
    "# root = pyrootutils.setup_root(\"/root/workspace/EP2\", pythonpath=True)\n",
    "cfg = omegaconf.OmegaConf.load(\"../configs/callbacks/eqprop_track.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omegaconf.OmegaConf.register_new_resolver(\n",
    "    \"double\", lambda condition, false_value: 2 * false_value if condition else false_value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.net.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(omegaconf.OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate\n",
    "\n",
    "instantiate(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra_zen\n",
    "\n",
    "hydra_zen.instantiate(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Hierachial configs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see https://github.com/mit-ll-responsible-ai/hydra-zen-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch as tr\n",
    "from hydra_zen import builds, instantiate, make_config, make_custom_builds_fn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from zen_model import UniversalFuncModule, single_layer_nn\n",
    "\n",
    "pbuilds = make_custom_builds_fn(zen_partial=True, populate_full_signature=True)\n",
    "\n",
    "OptimConf = pbuilds(Adam)\n",
    "\n",
    "LoaderConf = pbuilds(DataLoader, batch_size=25, shuffle=True, drop_last=True)\n",
    "\n",
    "ModelConf = builds(single_layer_nn, num_neurons=10)\n",
    "\n",
    "# configure our lightning module\n",
    "LitConf = pbuilds(\n",
    "    UniversalFuncModule,\n",
    "    model=ModelConf,\n",
    "    target_fn=tr.cos,\n",
    "    training_domain=builds(tr.linspace, start=-2 * math.pi, end=2 * math.pi, steps=1000),\n",
    ")\n",
    "\n",
    "TrainerConf = builds(pl.Trainer, max_epochs=100)\n",
    "\n",
    "ExperimentConfig = make_config(\n",
    "    optim=OptimConf,\n",
    "    dataloader=LoaderConf,\n",
    "    lit_module=LitConf,\n",
    "    trainer=TrainerConf,\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "\n",
    "def task_function(cfg):\n",
    "    # cfg: ExperimentConfig\n",
    "    pl.seed_everything(cfg.seed)\n",
    "\n",
    "    obj = instantiate(cfg)\n",
    "\n",
    "    # finish instantiating the lightning module, data-loader, and optimizer\n",
    "    lit_module = obj.lit_module(dataloader=obj.dataloader, optim=obj.optim)\n",
    "\n",
    "    # train the model\n",
    "    obj.trainer.fit(lit_module)\n",
    "\n",
    "    # evaluate the model over the domain to assess the fit\n",
    "    data = lit_module.training_domain\n",
    "    final_eval = lit_module.forward(data.reshape(-1, 1))\n",
    "    final_eval = final_eval.detach().cpu().numpy().ravel()\n",
    "\n",
    "    # return the final evaluation of our model:\n",
    "    # a shape-(N,) numpy-array\n",
    "    return final_eval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "\n",
    "@hydra.main(config_path=\"configs\", config_name=\"model/mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from src.models.components.eqprop_backbone import AnalogEP2\n",
    "from src.models.eqprop_module import EqPropLitModule\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def eqprop_model():\n",
    "    optimizer = optim.Adam(lr=0.001, weight_decay=0.0)\n",
    "    scheduler = lr_scheduler.ReduceLROnPlateau(mode=\"min\", factor=0.1, patience=10)\n",
    "    net = AnalogEP2(input_size=2, lin1_size=128, output_size=10)\n",
    "    model = EqPropLitModule(net, optimizer, scheduler)\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_optimizer(eqprop_model):\n",
    "    assert isinstance(eqprop_model.optimizer, optim.Adam)\n",
    "    assert eqprop_model.optimizer.defaults[\"lr\"] == 0.001\n",
    "    assert eqprop_model.optimizer.defaults[\"weight_decay\"] == 0.0\n",
    "\n",
    "\n",
    "def test_scheduler(eqprop_model):\n",
    "    assert isinstance(eqprop_model.scheduler, lr_scheduler.ReduceLROnPlateau)\n",
    "    assert eqprop_model.scheduler.mode == \"min\"\n",
    "    assert eqprop_model.scheduler.factor == 0.1\n",
    "    assert eqprop_model.scheduler.patience == 10\n",
    "\n",
    "\n",
    "def test_net(eqprop_model):\n",
    "    assert isinstance(eqprop_model.net, AnalogEP2)\n",
    "    assert eqprop_model.net.input_size == 2\n",
    "    assert eqprop_model.net.lin1_size == 128\n",
    "    assert eqprop_model.net.output_size == 10\n",
    "\n",
    "\n",
    "def test_double_input(eqprop_model):\n",
    "    assert eqprop_model.net.input_size == 4\n",
    "\n",
    "\n",
    "def test_double_output(eqprop_model):\n",
    "    assert eqprop_model.net.output_size == 20\n",
    "\n",
    "\n",
    "def test_positive_w(eqprop_model):\n",
    "    assert (eqprop_model.net.w > 0).all()\n",
    "\n",
    "\n",
    "def test_bias(eqprop_model):\n",
    "    assert eqprop_model.net.b is None\n",
    "\n",
    "\n",
    "def test_clip_weights(eqprop_model):\n",
    "    assert (eqprop_model.net.w.abs() <= 1).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a46654922a4325f3c08572184af70d13caa3bc46da3b244c5f3d5e106718b6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
