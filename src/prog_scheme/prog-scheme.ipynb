{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent-based programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Optional, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from src.utils.pylogger import RankedLogger\n",
    "\n",
    "log = RankedLogger(rank_zero_only=True)\n",
    "\n",
    "# TODO: realistic한가?\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_gdp(\n",
    "    self,\n",
    "    from_reference: bool = True,\n",
    "    x_values: Optional[Tensor] = None,\n",
    "    learning_rate: float = 1,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.01,\n",
    ") -> None:\n",
    "    \"\"\"Programm the target weights into the conductances using the\n",
    "    pulse update defined.\n",
    "\n",
    "    Programming is done using the defined tile-update (e.g. SGD)\n",
    "    and matching inputs (`x_values` by default `eye`).\n",
    "\n",
    "    Args:\n",
    "\n",
    "        from_reference: Whether to use weights from reference\n",
    "            (those that were initally set with `set_weights`) or\n",
    "            the current weights.\n",
    "        x_values: Values to use for the read-and verify. If none\n",
    "            are given, unit-vectors are used\n",
    "        learning_rate: Learning rate of the optimization\n",
    "        max_iter: max number of batches for the iterative programming\n",
    "        tolerance: Stop the iteration loop early if the mean\n",
    "            output deviation is below this number. Given in\n",
    "            relation to the max output.\n",
    "        w_init: initial weight matrix to start from. If given as\n",
    "            float, weights are set uniform random in `[-w_init,\n",
    "            w_init]`. This init weight is given directly in\n",
    "            normalized conductance units and should include the\n",
    "            bias row if existing.\n",
    "    \"\"\"\n",
    "\n",
    "    if not from_reference or self.reference_combined_weights is None:\n",
    "        self.reference_combined_weights = self.tile.get_weights()\n",
    "        target_weights = self.reference_combined_weights\n",
    "\n",
    "    if x_values is None:\n",
    "        x_values = torch.eye(self.tile.get_x_size())\n",
    "        # x_values = torch.rand(self.tile.get_x_size(), self.tile.get_x_size())\n",
    "        x_values = x_values.to(self.device)\n",
    "        target_values = x_values @ target_weights.to(self.device).T\n",
    "\n",
    "    target_max = target_values.abs().max().item()\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    self.tile.set_learning_rate(learning_rate)  # type: ignore\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        y = self.tile.forward(x_values, False)\n",
    "        # TODO: error와 weight 2norm 사이 관계 분석\n",
    "        error = y - target_values\n",
    "        err_normalized = error.abs().mean().item() / target_max\n",
    "        mtx_diff = self.tile.get_weights() - target_weights\n",
    "        l2_norm = torch.linalg.matrix_norm(mtx_diff, ord=\"nuc\")\n",
    "        log.debug(f\"Error: {l2_norm}\")\n",
    "        # log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and err_normalized < tolerance:\n",
    "            break\n",
    "        self.tile.update(x_values, error, False)  # type: ignore\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = RankedLogger()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_gdp2(\n",
    "    self,\n",
    "    batch_size: int = 5,\n",
    "    learning_rate: float = 1,\n",
    "    max_iter: int = 100,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.01,\n",
    ") -> None:\n",
    "    \"\"\"Programm the target weights into the conductances using the\n",
    "    pulse update defined.\n",
    "\n",
    "    Variable batch version of the `program_weights_gdp` method.\n",
    "    \"\"\"\n",
    "    target_weights = self.tile.get_weights()\n",
    "\n",
    "    input_size = self.tile.get_x_size()\n",
    "    x_values = torch.eye(input_size)\n",
    "    x_values = x_values.to(self.device)\n",
    "    target_values = x_values @ target_weights.to(self.device).T\n",
    "\n",
    "    target_max = target_values.abs().max().item()\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    self.tile.set_learning_rate(learning_rate)  # type: ignore\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "\n",
    "        if end_idx > len(x_values):\n",
    "            # Calculate how much we exceed the length\n",
    "            exceed_length = end_idx - len(x_values)\n",
    "\n",
    "            # Slice the arrays and concatenate the exceeded part from the beginning\n",
    "            x = torch.concatenate((x_values[start_idx:], x_values[:exceed_length]))\n",
    "            target = torch.concatenate((target_values[start_idx:], target_values[:exceed_length]))\n",
    "        else:\n",
    "            x = x_values[start_idx:end_idx]\n",
    "            target = target_values[start_idx:end_idx]\n",
    "\n",
    "        y = self.tile.forward(x, False)\n",
    "        error = y - target\n",
    "        err_normalized = error.abs().mean().item() / target_max\n",
    "        mtx_diff = self.tile.get_weights() - target_weights\n",
    "        nuc_norm = torch.linalg.matrix_norm(mtx_diff, ord=\"nuc\")\n",
    "        log.debug(f\"Error: {nuc_norm}\")\n",
    "        # log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and nuc_norm < tolerance:\n",
    "            break\n",
    "        self.tile.update(x, error, False)  # type: ignore\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed method(SVD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compensate_half_selection(v: Tensor) -> Tensor:\n",
    "    \"\"\"Compensate the half-selection effect for a vector.\n",
    "\n",
    "    Args:\n",
    "        v: Vector to compensate.\n",
    "\n",
    "    Returns:\n",
    "        Compensated vector.\n",
    "    \"\"\"\n",
    "    return v\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def program_weights_svd(\n",
    "    self,\n",
    "    max_iter: int = 100,\n",
    "    use_rank_as_criterion: bool = False,\n",
    "    tolerance: Optional[float] = 0.01,\n",
    "    w_init: Union[float, Tensor] = 0.0,\n",
    "    rank_atol: Optional[float] = 1e-2,\n",
    "    svd_once: bool = False,\n",
    "    **kwargs: Any,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform singular value decomposition (SVD) based weight programming.\n",
    "\n",
    "    Args:\n",
    "        use_rank_as_criterion (bool, optional): Use rank as stopping criterion. If False, use max_iter. Defaults to False.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 100.\n",
    "        tolerance (float, optional): Tolerance for convergence. Defaults to 0.01.\n",
    "        w_init (Union[float, Tensor], optional): Initial value for weights. Defaults to 0.01.\n",
    "        rank_atol (float, optional): Absolute tolerance for numerical rank computation. Defaults to 1e-6.\n",
    "        rank_rtol (float, optional): Relative tolerance for numerical rank computation. Defaults to 1e-6.\n",
    "        svd_once (bool, optional): Flag indicating whether to perform SVD only once. Defaults to False.\n",
    "        **kwargs: Additional keyword arguments.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    target_weights = self.tile.get_weights()\n",
    "    # target_weights = self.tile.get_weights() if target_weights is None else target_weights\n",
    "\n",
    "    if isinstance(w_init, Tensor):\n",
    "        self.tile.set_weights(w_init)\n",
    "    else:\n",
    "        self.tile.set_weights_uniform_random(-w_init, w_init)  # type: ignore\n",
    "\n",
    "    lr_save = self.tile.get_learning_rate()  # type: ignore\n",
    "    # x_values = torch.eye(self.tile.get_x_size())\n",
    "    # x_values = x_values.to(self.device)\n",
    "    # target_values = x_values @ target_weights.to(self.device).T\n",
    "    # target_max = target_values.abs().max().item()\n",
    "    self.tile.set_learning_rate(1)  # type: ignore\n",
    "    # since tile.update() updates w -= lr*delta_w so flip the sign\n",
    "    diff = self.tile.get_weights() - target_weights\n",
    "    # normalize diff matrix\n",
    "    U, S, Vh = torch.linalg.svd(diff.double(), full_matrices=False)\n",
    "    # rank = torch.linalg.matrix_rank(diff)\n",
    "    if rank_atol is None:\n",
    "        rank_atol = S.max() * max(diff.shape) * torch.finfo(S.dtype).eps\n",
    "\n",
    "    rank = torch.sum(S > rank_atol).item()\n",
    "    i = 0\n",
    "    max_iter = min(max_iter, rank) if use_rank_as_criterion else max_iter\n",
    "    for _ in range(max_iter):\n",
    "        u = U[:, i]\n",
    "        v = Vh[i, :]\n",
    "        # uv_ratio = torch.sqrt(u/v)\n",
    "        uv_ratio = 1\n",
    "        sqrt_s = torch.sqrt(S[i])\n",
    "        v *= uv_ratio * sqrt_s\n",
    "        u *= sqrt_s / uv_ratio\n",
    "        u1, v1 = compensate_half_selection(u), compensate_half_selection(v)\n",
    "        self.tile.update(v1.float(), u1.float(), False)\n",
    "\n",
    "        # TODO: self.get_weights()\n",
    "        diff = self.tile.get_weights() - target_weights\n",
    "        U, S, Vh = torch.linalg.svd(diff.double(), full_matrices=False)\n",
    "        nuc_norm = S.sum() if svd_once else torch.linalg.matrix_norm(diff, ord=\"nuc\")\n",
    "        log.debug(f\"Error: {nuc_norm}\")\n",
    "        # y = self.tile.forward(x_values, False)\n",
    "        # # TODO: error와 weight 2norm 사이 관계 분석\n",
    "        # error = y - target_values\n",
    "        # err_normalized = error.abs().mean().item() / target_max\n",
    "        # log.debug(f\"Error: {err_normalized}\")\n",
    "        if tolerance is not None and nuc_norm < tolerance:\n",
    "            break\n",
    "        elif svd_once:\n",
    "            i += 1\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    self.tile.set_learning_rate(lr_save)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form log list[string], search Error: {err_normalized} pattern and extract the value\n",
    "# into list\n",
    "import re\n",
    "\n",
    "from src.utils.logging_utils import LogCapture\n",
    "\n",
    "\n",
    "def extract_error(log_list, prefix: str = \"Error: \") -> list:\n",
    "    err_list = []\n",
    "\n",
    "    for log in log_list:\n",
    "        if prefix in log:\n",
    "            err_list.append(float(re.findall(prefix + r\"([0-9.e-]+)\", log)[0]))\n",
    "\n",
    "    return err_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def plot_singular_values(Ws: tuple[torch.Tensor]):\n",
    "    for w in Ws:\n",
    "        s = torch.linalg.svdvals(w)\n",
    "        plt.plot(s)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Singular Value Index\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.title(\"Singular Values of Weight Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_size = 100\n",
    "output_size = 50\n",
    "batch_size = 1\n",
    "\n",
    "# generate low rank matrix\n",
    "rank = 50\n",
    "w = torch.randn(input_size, output_size).double()\n",
    "w.clamp_(-1, 1)\n",
    "if rank < min(w.shape):\n",
    "    u, s, v = torch.svd(w)\n",
    "    w = torch.mm(u[:, :rank], torch.mm(torch.diag(s[:rank]), v[:, :rank].t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnalogTile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.configs import FloatingPointRPUConfig, SingleRPUConfig\n",
    "from aihwkit.simulator.configs.devices import (\n",
    "    ConstantStepDevice,\n",
    "    DriftParameter,\n",
    "    ExpStepDevice,\n",
    "    FloatingPointDevice,\n",
    "    IdealDevice,\n",
    "    SimpleDriftParameter,\n",
    ")\n",
    "from aihwkit.simulator.configs.utils import (\n",
    "    InputRangeParameter,\n",
    "    PrePostProcessingParameter,\n",
    "    UpdateParameters,\n",
    ")\n",
    "from aihwkit.simulator.parameters.enums import PulseType\n",
    "from aihwkit.simulator.presets.configs import IdealizedPreset, PCMPreset\n",
    "from aihwkit.simulator.presets.devices import IdealizedPresetDevice\n",
    "from aihwkit.simulator.tiles import FloatingPointTile\n",
    "from aihwkit.simulator.tiles.analog import AnalogTile\n",
    "\n",
    "pre_post_cfg = PrePostProcessingParameter(input_range=InputRangeParameter(enable=True))\n",
    "# device_cfg = IdealDevice()\n",
    "device_cfg = ConstantStepDevice()\n",
    "update_cfg = UpdateParameters(pulse_type=PulseType.STOCHASTIC_COMPRESSED)\n",
    "\n",
    "# rpuconfig = SingleRPUConfig(update=update_cfg, device=device_cfg, pre_post=pre_post_cfg)\n",
    "# rpuconfig = FloatingPointRPUConfig()\n",
    "rpuconfig = SingleRPUConfig(device=device_cfg, update=update_cfg)\n",
    "rpuconfig.forward.is_perfect = True\n",
    "rpuconfig.forward.out_noise = 0.0\n",
    "# rpuconfig = IdealizedPreset()\n",
    "atile = AnalogTile(output_size, input_size, rpu_config=rpuconfig)  # without periphery\n",
    "atile_dic = {}\n",
    "atile.state_dict(atile_dic)\n",
    "atile2 = AnalogTile(output_size, input_size, rpu_config=rpuconfig)\n",
    "atile2.load_state_dict(atile_dic, assign=True)\n",
    "print(rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpuconfig.device.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.tiles.periphery import TileWithPeriphery\n",
    "\n",
    "# enroll the programming methods\n",
    "atile.program_weights = program_weights_gdp2.__get__(atile, TileWithPeriphery)\n",
    "atile2.program_weights = program_weights_svd.__get__(atile2, TileWithPeriphery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    atile.tile.set_weights(w.clone().T)\n",
    "    atile.program_weights(batch_size=batch_size, tolerance=1e-10, max_iter=1000)\n",
    "    log_list1 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    # atile2.set_weights(weight=w, realistic=True)\n",
    "    atile2.tile.set_weights(w.clone().T)\n",
    "    atile2.program_weights(\n",
    "        max_iter=1000, tolerance=1e-10, svd_once=False, target_weights=w.clone().T\n",
    "    )\n",
    "    log_list2 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = (w.T - atile.tile.get_weights(), w.T - atile2.tile.get_weights())\n",
    "# plot_singular_values(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\" nulcear norm of atile: {torch.linalg.matrix_norm(W[0], ord='nuc')}, atile2: {torch.linalg.matrix_norm(W[1], ord='nuc')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list2[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_list1 = extract_error(log_list1)\n",
    "err_list2 = extract_error(log_list2)\n",
    "plt.semilogy(err_list1)\n",
    "plt.semilogy(err_list2)\n",
    "# set legend\n",
    "plt.legend([f\"gdp-seq(batchsize {batch_size})\", \"svd\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Nuclear norm of weight error (sum of singular values)\")\n",
    "plt.title(\"Error vs Iteration @ {}x{}, rank={}\".format(input_size, output_size, rank))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GDP batch-size effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size_ in [1, 5, 10, 20, 50, input_size]:\n",
    "    with LogCapture() as logc:\n",
    "        atile.tile.set_weights(w.T)\n",
    "        atile.program_weights(batch_size=batch_size_)\n",
    "        log_list = logc.get_log_list()\n",
    "    err_list = extract_error(log_list)\n",
    "    num_iter = len(err_list)\n",
    "    plt.semilogy(err_list, label=f\"batch_size={batch_size_}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Nuclear norm of weight error\")\n",
    "plt.title(\n",
    "    \"{}x{} rank={} matrix with {}\".format(\n",
    "        input_size, output_size, rank, atile.rpu_config.device.__class__.__name__\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d2d variaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dataclass fields\n",
    "atile.rpu_config.device.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.T[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the element wise perturbation is applied\n",
    "\n",
    "atile.tile.set_weights(w.T)\n",
    "wtile = atile.tile.get_weights()\n",
    "torch.allclose(wtile, w.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomTile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.simulator.tiles.custom import CustomTile\n",
    "\n",
    "ctile = CustomTile(output_size, input_size)\n",
    "ctile.get_weights(realistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RealisticTile(Ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.prog_scheme.realistic import RealisticTile, RPUConfigwithProgram\n",
    "\n",
    "rpu_config = RPUConfigwithProgram(program_weights=program_weights_gdp)\n",
    "ctile = RealisticTile(output_size, input_size, rpu_config=rpu_config)\n",
    "\n",
    "rpu_config2 = RPUConfigwithProgram(program_weights=program_weights_svd)\n",
    "ctile2 = RealisticTile(output_size, input_size, rpu_config=rpu_config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rpu_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with LogCapture() as logc:\n",
    "    ctile.set_weights(w, realistic=True)\n",
    "    log_list = logc.get_log_list()\n",
    "\n",
    "with LogCapture() as logc:\n",
    "    ctile2.set_weights(w, realistic=True)\n",
    "    log_list2 = logc.get_log_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract error and plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "err_list = extract_error(log_list)\n",
    "err_list2 = extract_error(log_list2)\n",
    "\n",
    "plt.plot(err_list, label=\"gpc\")\n",
    "plt.plot(err_list2, label=\"svd\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.title(\"Error vs Iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "only `AnalogTile` which inherits `TileWithPeriphery` class has `program_weights` method\n",
    "\n",
    "`program_weights` method implements \"Gradient descent-based programming of analog in-memory computing cores\" by default\n",
    "\n",
    "`set_weights` method is used to set the weights of the analog tile to the given values\\\n",
    "`program_weights` method is internally called by `set_weights` method to program the weights of the analog tile\\\n",
    "\n",
    "`get_weights` method is used to get the weights of the analog tile\\\n",
    "`read_weights` method is used to read the weights of the analog tile with read noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aihwkit.nn import AnalogLinear\n",
    "from aihwkit.optim import AnalogSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digital_layer = torch.nn.Linear(input_size, output_size, bias=False)\n",
    "layer = AnalogLinear.from_digital(digital_layer, rpuconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AnalogSGD(layer.parameters(), lr=0.005)\n",
    "losses = []\n",
    "for _ in range(1000):\n",
    "    x = torch.rand(input_size)\n",
    "    yhat = layer(x)\n",
    "    loss = (yhat**2).sum()\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
