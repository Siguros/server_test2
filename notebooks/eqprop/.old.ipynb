{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.core.spice import ShallowCircuit, SPICEParser, XyceSim, create_circuit\n",
    "from src.utils.eqprop_utils import AdjustParams, deltaV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_XOR():\n",
    "    \"\"\"load XOR dataset\n",
    "\n",
    "    Returns:\n",
    "        TensorDataset: _description_\n",
    "    \"\"\"\n",
    "    from torch.utils.data import TensorDataset\n",
    "\n",
    "    X = torch.FloatTensor([[[-2, -2]], [[-2, 2]], [[2, -2]], [[2, 2]]])\n",
    "    Y = torch.FloatTensor([[1, 0], [0, 1], [1, 0], [0, 1]])\n",
    "    dataset = TensorDataset(X, Y)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_train(\n",
    "    circuit: ShallowCircuit,\n",
    "    dimensions: list,\n",
    "    batch,\n",
    "    beta,\n",
    "    mpi_commands=None,\n",
    "    normalize_per_input: bool = False,\n",
    "):  # id:int):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        circuit (Circuit): _description_\n",
    "        dimensions (list): _description_\n",
    "        x (_type_): _description_\n",
    "        y (_type_): _description_\n",
    "        mpi_commands (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        free_Vdrops (list): _description_\n",
    "        nudged_Vdrops (list): _description_\n",
    "        loss (float): _description_\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    x, y = batch\n",
    "    # normalize input with torch.var_mean()\n",
    "    if normalize_per_input:\n",
    "        (std, mean) = torch.std_mean(x)\n",
    "        x = (x - mean) / std\n",
    "\n",
    "    # if mpi_commands[-1] == \"-cpu-set\":\n",
    "    #     mpi_commands.append(str(id + 2))\n",
    "    # free phase\n",
    "    SPICEParser.clampLayer(circuit, x)\n",
    "    # analysis\n",
    "    xyce = XyceSim(mpi_command=mpi_commands)\n",
    "    raw_file = xyce(spice_input=circuit)\n",
    "    voltages = SPICEParser.fastRawfileParser(\n",
    "        raw_file, nodenames=circuit.nodes, dimensions=dimensions\n",
    "    )\n",
    "    free_Vdrops = []\n",
    "    ypred = None\n",
    "    for vin, vout in voltages:\n",
    "        free_Vdrops.append(deltaV(vin, vout))\n",
    "        ypred = vout\n",
    "\n",
    "    # calculate output layer grads\n",
    "\n",
    "    ypred = ypred.expand(1, -1)\n",
    "    ypred.requires_grad = True\n",
    "    loss = F.mse_loss(ypred, y.expand(1, -1).double(), reduction=\"sum\")\n",
    "    # loss = costFun.compute_energy(ypred)\n",
    "    loss.backward()\n",
    "    ygrad = ypred.grad * beta\n",
    "    # nudged phase\n",
    "    SPICEParser.releaseLayer(circuit, ygrad)\n",
    "    # analysis 2\n",
    "    raw_file2 = xyce(spice_input=circuit)\n",
    "    voltages2 = SPICEParser.fastRawfileParser(\n",
    "        raw_file2, nodenames=circuit.nodes, dimensions=dimensions\n",
    "    )\n",
    "    nudged_Vdrops = [deltaV(vin, vout) for (vin, vout) in voltages2]\n",
    "\n",
    "    return (free_Vdrops, nudged_Vdrops, np.abs(loss.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ray_predict(circuit: ShallowCircuit, dimensions: list, X, mpi_commands=None):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        circuit (Circuit): _description_\n",
    "        dimensions (list): _description_\n",
    "        x (Tensor): input tensor\n",
    "        mpi_commands (_type_, optional): _description_. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tensor(1,num_classes): prediction per classes\n",
    "    \"\"\"\n",
    "    x = X\n",
    "    # input_dimension = dimensions[0]\n",
    "    # output_dimension = dimensions[-1]\n",
    "    # if mpi_commands[-1] == \"-cpu-set\":\n",
    "    #     mpi_commands.append(str(id + 2))\n",
    "\n",
    "    # free phase\n",
    "    SPICEParser.clampLayer(circuit, x)\n",
    "    # analysis\n",
    "    xyce = XyceSim(mpi_command=mpi_commands)\n",
    "    raw_file = xyce(spice_input=circuit)\n",
    "    (_, Vout) = SPICEParser.fastRawfileParser(\n",
    "        raw_file, nodenames=circuit.nodes, dimensions=dimensions\n",
    "    )\n",
    "    out = Vout[-1][::2] - Vout[-1][1::2]\n",
    "    out = torch.from_numpy(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningRay(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims: list,\n",
    "        batch_size: int,\n",
    "        num_classes: int,\n",
    "        SPICE_params: dict,\n",
    "        mpi_commands: list,\n",
    "        optim_kwargs: dict = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dimensions = dims\n",
    "        self.n_layers = len(self.dimensions)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.optim_kwargs = optim_kwargs\n",
    "        # hparams\n",
    "        self.SPICE_params = SPICE_params\n",
    "        self.save_hyperparameters(ignore=[\"mpi_commands\"])\n",
    "        self._hparams_tolist()\n",
    "\n",
    "        # weight initialize\n",
    "        self._weight_init()\n",
    "\n",
    "        self.Pycircuit = create_circuit(W=self.W, dimensions=self.dimensions, **self.SPICE_params)\n",
    "        self.circuit = ShallowCircuit.copyFromCircuit(self.Pycircuit)\n",
    "\n",
    "        # manual optimization\n",
    "        self.automatic_optimization = False\n",
    "        self.mpi_commands = mpi_commands  # delete '--allow-run-as-root'\n",
    "\n",
    "        if batch_size == 1:\n",
    "            self.mpi_commands.pop()\n",
    "            self.mpi_commands.pop()\n",
    "            self.mpi_commands.append(str(os.cpu_count() - 2))\n",
    "            self.mpi_commands.append(\"-cpu-set\")\n",
    "            self.mpi_commands.append(\"2-\" + str(os.cpu_count() - 1))\n",
    "            print(f\"batch size 1 detected. change mpi cmd as {self.mpi_commands}\")\n",
    "        self.clipper = AdjustParams(L=self.SPICE_params[\"L\"], U=None)\n",
    "\n",
    "    def _hparams_tolist(self, keys=(\"alpha\", \"L\", \"U\")) -> None:\n",
    "        \"\"\"broadcast hyperparameters to list\n",
    "\n",
    "        Args:\n",
    "            keys (tuple, optional): _description_. Defaults to ('alpha', 'L', 'U').\n",
    "        \"\"\"\n",
    "        assert self.SPICE_params is not None, \"hparams not set\"\n",
    "        [\n",
    "            self.SPICE_params.update({key: [val] * (self.n_layers - 1)})\n",
    "            for (key, val) in self.SPICE_params.items()\n",
    "            if key in keys and type(val) is not list\n",
    "        ]\n",
    "        assert (\n",
    "            len(self.SPICE_params[\"alpha\"]) == self.n_layers - 1\n",
    "        ), \"alpha length does not match n_layers\"\n",
    "\n",
    "    def _weight_init(self) -> None:\n",
    "        assert self.dimensions is not None, \"dimensions not set\"\n",
    "        assert self.SPICE_params is not None, \"hyper_params not set\"\n",
    "        self.W = nn.ModuleList(\n",
    "            nn.Linear(dim1, dim2, bias=False)  # include bias in weight\n",
    "            for dim1, dim2 in zip(self.dimensions[:-1], self.dimensions[1:])\n",
    "        )\n",
    "        assert self.SPICE_params[\"L\"] is not None, \"L not set\"\n",
    "        assert self.SPICE_params[\"U\"] is not None, \"U not set\"\n",
    "        for module, Li, Ui in zip(self.W, self.SPICE_params[\"L\"], self.SPICE_params[\"U\"]):\n",
    "            module.weight.data = nn.init.uniform_(module.weight, Li, Ui)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = []\n",
    "        [\n",
    "            params.append({\"params\": W.parameters(), \"lr\": self.SPICE_params[\"alpha\"][idx]})\n",
    "            for idx, W in enumerate(self.W)\n",
    "        ]\n",
    "        optimizer = torch.optim.SGD(params, **self.optim_kwargs)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        beta_i = self.SPICE_params[\"beta\"] * [-1, 1][torch.randint(0, 2, (1,))]  # random sign\n",
    "\n",
    "        # clone circuit, processed data\n",
    "        # circuit = ray.put(self.circuit)\n",
    "        # batch = ray.put(self._data_preprocess(batch, num_classes=self.num_classes))\n",
    "        # costFun = ray.put(self.net.costFun)\n",
    "        # parallel processing with ray\n",
    "        # maybe with loss?\n",
    "        Vlists = ray_train(\n",
    "            self.circuit, self.dimensions, batch, beta=beta_i, mpi_commands=self.mpi_commands\n",
    "        )\n",
    "        # merge Vlists\n",
    "        self.fdV, self.ndV, losses = zip(*Vlists)\n",
    "\n",
    "        # fdVtup\n",
    "\n",
    "        # self.fdV = np.array(list(fdVmap), dtype=object) / self.batch_size\n",
    "        # ndVmap = reduce(lambda x, y: map(add, x, y), ndVtuple)\n",
    "        # self.ndV = np.array(list(ndVmap), dtype=object) / self.batch_size\n",
    "\n",
    "        # update everything\n",
    "        # update G\n",
    "        # self.net.w_optimize(fdV, ndV, self.optimizers())\n",
    "        self.zero_grad()\n",
    "        opt = self.optimizers()\n",
    "        opt.zero_grad()\n",
    "        for p, fdv, ndv in zip(self.parameters(), self.fdV, self.ndV):\n",
    "            p.grad = -(1 / beta_i) * torch.from_numpy(ndv**2 - fdv**2).transpose(1, 0).float()\n",
    "            p.grad.contiguous()\n",
    "\n",
    "        # clip weights(G)\n",
    "        opt.step()\n",
    "        self.clipper(self.W)\n",
    "        # update Rarray\n",
    "        SPICEParser.updateWeight(self.circuit, self.W)\n",
    "\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch, to the progress bar and logger\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            np.array(losses).mean(),\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        beta_i = self.SPICE_params[\"beta\"] * [-1, 1][torch.randint(0, 2, (1,))]\n",
    "        (X, Y) = self._data_preprocess(batch, num_classes=self.num_classes)\n",
    "\n",
    "        outList = ray_predict(\n",
    "            circuit=self.circuit, dimensions=self.dimensions, X=X, mpi_commands=self.mpi_commands\n",
    "        )\n",
    "        outs = torch.stack(outList, dim=0)\n",
    "        o1 = outs.clone().detach()\n",
    "        # calculate output layer grads\n",
    "        outs.requires_grad = True\n",
    "        # ypreds = F.softmax(outs, dim=1)\n",
    "        loss = F.mse_loss(outs, Y, reduction=\"sum\")\n",
    "        acc = self.accuracy(outs, Y)\n",
    "        self.log(\n",
    "            \"valid_loss\", torch.abs(loss), on_step=True, on_epoch=True, prog_bar=False, logger=True\n",
    "        )\n",
    "        self.log(\"valid_acc\", acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"confidence\", torch.max(o1, dim=1)[0].mean(), on_epoch=True, logger=True)\n",
    "\n",
    "    # def validation_epoch_end(self, output):\n",
    "    #     [self.tb.add_histogram(name, param, self.current_epoch)for name, param in self.named_parameters()]\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        pass\n",
    "\n",
    "    # def predict_step(self, batch, batch_idx):\n",
    "    #     circuit = ray.put(self.circuit)\n",
    "    #     (X, _) = self._data_preprocess(batch, num_classes=self.num_classes)\n",
    "    #     ypredList = ray_predict(id, circuit=circuit, dimensions=self.dimensions, X=X, mpi_commands=self.mpi_commands)\n",
    "    #     ypreds = torch.stack(ypredList, dim=0)\n",
    "    #     return torch.argmax(ypreds, dim=1)\n",
    "\n",
    "    def accuracy(self, ypreds, labels):\n",
    "        _, predicted = torch.max(ypreds.data, 1)\n",
    "        correct = (predicted == torch.argmax(labels, 1)).sum().item()\n",
    "        accuracy = correct / len(labels)\n",
    "        return torch.tensor(accuracy)\n",
    "\n",
    "    @staticmethod\n",
    "    def _data_preprocess(batch, num_classes=10):\n",
    "        \"\"\"process batch data to fit in analog circuit model\n",
    "\n",
    "        Args:\n",
    "            batch (_type_): batch dataset\n",
    "            num_classes (int, optional): number of classification classes. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            _type_: processed batch\n",
    "        \"\"\"\n",
    "        X_batch, Y_batch = batch\n",
    "        X_batch = X_batch.view(X_batch.size(0), -1)  # == X_batch.view(-1,X_batch.size(-1)**2)\n",
    "        X_batch = X_batch.repeat_interleave(2, dim=1)\n",
    "        X_batch[:, 1::2] = -X_batch[:, ::2]\n",
    "        if Y_batch.size(-1) != num_classes:\n",
    "            Y_batch = F.one_hot(Y_batch, num_classes=num_classes).expand(Y_batch.size(0), -1)\n",
    "        return (X_batch, Y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPICE_params = {\n",
    "    \"L\": 1e-7,\n",
    "    \"U\": None,\n",
    "    \"A\": 4,\n",
    "    \"alpha\": [0.1, 0.05],  # ~learning rate\n",
    "    \"beta\": 1e-2,\n",
    "    \"Diode\": {\n",
    "        \"Path\": \"/path/to/libraries/diode/switching/1N4148.lib\",\n",
    "        \"ModelName\": \"1N4148\",\n",
    "        \"Rectifier\": \"BidRectifier\",\n",
    "    },\n",
    "    \"noise\": 0,  # ratio\n",
    "}\n",
    "mpi_commands = [\"mpirun\", \"-use-hwthread-cpus\", \"-np\", \"1\", \"-cpu-set\"]\n",
    "cfg = {\n",
    "    \"frac\": 1,\n",
    "    \"num_epochs\": 10,\n",
    "    \"upper_frac\": 2.8,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"std_dev\": 1,\n",
    "    \"SPICE_params/A\": 4,\n",
    "    \"SPICE_params/Diode/Rectifier\": \"BidRectifier\",\n",
    "    \"SPICE_params/L\": 1e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_XOR()\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=1,\n",
    "    persistent_workers=False,\n",
    "    batch_size=1,\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=1,\n",
    "    persistent_workers=False,\n",
    "    batch_size=1,\n",
    "    drop_last=True,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightningRay(**cfg.to_dict())\n",
    "L.seed_everything(42)\n",
    "trainer = L.Trainer(max_epochs=cfg[\"num_epochs\"])\n",
    "trainer.fit(model, rain_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
